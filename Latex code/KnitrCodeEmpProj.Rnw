\documentclass[12pt,a4paper]{article}      % document type

\usepackage[margin=1in]{geometry}          % customize page layout (the text is large: 14.7cm, 418.25pt, 41.83em ) [top=8em,bottom=8em]
\usepackage{setspace}                      % allow line spacing
\setstretch{1.5}                           % line spacing
\setlength\parindent{0em}                  % line indentation

\usepackage[T1]{fontenc}                   % 8-bit font encoding
\usepackage{xcolor}                        % add colors
\usepackage{moresize}                      % add \ssmall and \HUGE
\usepackage{mathpazo}                      % palatino font
%\usepackage{charter}                      % charter font
\usepackage{microtype}                     % improve general appearance
\usepackage{booktabs}                      % improve tables quality
\widowpenalty 10000                        % avoid widows
\clubpenalty 10000                         % avoid orphans

\usepackage{amsmath}                       % mathematical typesetting
\usepackage{amssymb, latexsym}             % import math symbols
\usepackage{textcomp}                      % import text symbols
\usepackage{enumerate}                     % allow enumerate counter styles
\usepackage{graphicx}                      % manage pictures
\usepackage{subfig}                        % allow subfigures
\usepackage{adjustbox}                     % add macros to adjust boxed content
\usepackage{tikz, tikz-qtree}              % create graphic elements
\usepackage{pdflscape}                     % allow landscape mode 
\usepackage[hidelinks,
            bookmarks=FALSE]{hyperref}     % use hyperlinks
\usepackage{url}                           % write url with all their characters
\usepackage{natbib}                        % bibliografy support (use \citet{key} or \citep{key})
%\usepackage{apacite}                      % Americ.Psych.Assoc. citations style
\renewcommand{\bibname}{References}        % choose name for reference section

\usepackage{rotating}

%------------------------------------------------------------------%
\begin{document}
%------------------------------------------------------------------%

\pagenumbering{gobble}  % avoid numbering of cover page

%\title{Econometrics of Program Evaluation}
%\maketitle

\vspace{1cm}
\begin{center}
\text{\Large{Toulouse School of Economics}} \\
\vspace{2cm}
\text{\large{Master 2 in Econometrics and Empirical Economics}} \\
\vspace{1cm}
\text{\large{Empirical Project}} \\  % Econometric Production Analysis and Efficiency Analysis
\vspace{1cm}
\textbf{\Large{A production function and stochastic frontier analysis of the World Health Organization's panel data on national health care systems}} \\
\vspace{1cm}
Jos\'{e} Alvarez, Nicola Benigni, Irina Cotovici, Igor Custodio Jo\~{a}o \\
\vspace{1cm}
Supervised by: \\
Professor Catherine Cazals \\
\vspace{1cm}
March 22, 2017
\end{center}

\vfill

% \begin{abstract}
% \noindent
% The abstract goes here...
% \end{abstract}

%------------------------------------------------------------------%
\newpage 
\pagenumbering{arabic}
\tableofcontents
%\listoffigures
%\listoftables
%------------------------------------------------------------------%

\newpage
\section{Introduction}

The national health care system of a country plays a fundamental role in its economy. One can reasonably expect, for example, that a healthy working population will be more productive throughout its active years than an unhealthy one, as the former is less likely to fall sick than the latter. It should not come as a surprise that the healthiest countries also tend to have the "healthiest" economies. It is in a country's best interest to achieve a functioning and efficient health care system. Though the two concepts are not necessarily interchangeable: all efficient health care systems are indeed functional, but not all functioning health care systems are efficient. This paper explores the latter point.

Using the World Health Organization's panel data on national care systems, we carry out first an econometric production function analysis and then an efficiency analysis of these countries' national health care systems. The paper is organized in two parts. In \textbf{Part I}, we develop a model specification for the production function of the panel data and carry out a production analysis. In \textbf{Part II}, we proceed to do a stochastic frontier analysis of the panel data.

\subsection{Literature review}
\subsubsection{Production function}

In its simplest form, we can define a production function for firm $i$ as
$$
\begin{aligned}
y_i = f(x_i)
\end{aligned}
$$
where $y_i$ is a given output, $x_i$ is the input (or the collection of inputs) needed for producing the given output, and $f(.)$ is the function that defines the relationship between the given output and its input(s). The idea can be easily extended to the context of this paper: the national health care system of a country is the output and the national investment per capita on health infrastructure, for example, is one of the inputs. Similarly, $f(.)$ would determine how exactly that investment on health infrastructure translates into a better health care system. In this case rather than having firm $i$, we have country $i$.

We can extend the above setting by including an error term
$$
\begin{aligned}
y_i & = f(x_i) + u_i \\
\epsilon_i & \overset{iid}{\sim} \mathcal{N}(0, \sigma ^2)
\end{aligned}
$$
where $u_i$ introduces randomness (i.e. noise) into the production function. For example, the sudden outburst of a violent conflict in a country would damage its health care system despite the fact that the conflict would have nothing to do with the health care production process itself. By assuming $\mathbb{E}[x'\epsilon]=0$, the above setting becomes the the mean production function model, as we have $\mathbb{E}[y_i]=\mathbb{E}[f(x_i)]$.

Following Schmidt and Sickles (1984) and Greene (2004), we extend the above mean production function model to account for the panel data nature of the sample and thus we denote the production function as
$$
\begin{aligned}
y_i & = f(x_i) + u_{it} \\
u_{it} & = \alpha_i + \epsilon_{it} \\
\epsilon_{it} & \overset{iid}{\sim} \mathcal{N}(0, \sigma ^2)
\end{aligned}
$$
where both input(s) and output are indexed by country $i$ at time $t$. The composite error term, $u_{it}$, consists of two elements: an idiosyncratic time-variant, individual-specific error term $\epsilon_{it}$ and the time-invariant, country-specific characteristic $\alpha_i$, which captures unobserved heterogeneity. Both are unobserved to the econometrician, but it is $\alpha_i$ that threatens the estimation of the model. As in the cross-sectional setting, we assume $\mathbb{E}[x' \epsilon_{it}]=0$ to complete our mean production function model. Regarding $\alpha_i$, however, we can tackle it in a Fixed Effects (FE) framework, where $\mathbb{E}(x' \alpha) \neq 0$, or in a Random Effects (RE) framework, where $\mathbb{E}(x' \alpha) = 0$. Both approaches can only be done in a panel data setting. In the case of cross sectional analysis, there exists the risk that the unobserved country-specific heterogeneity is creating measurement error and thus biased estimators. 

In Greene (2004), $\alpha_i$ is treated as a measure of a country's production inefficiency that is unobserved to the econometrician. The idea behind it is that only country $i$ knows exactly how efficient or inefficient it is. Greene argues that what might seem unobserved heterogeneity might actually be country-specific inefficiency. Greene goes beyond the traditional FE and RE models by using a more flexible model: the true random effect model. Although we do not use this model, it is important to keep in mind Greene's point that unobserved heterogeneity might be unmeasured inefficiency.

So farwe have not specified the functional form of the production function, $f(.)$. The standard production function involves a parametrically defined family of functions. Henningsen (2014) provides a good survey of these families of functions, the most popular one being the Cobb-Douglas production function. Less traditional production functions or, in other words, less restrictive models can also be used in estimating production functions (Afriat, 1972). One of these approaches is using nonparametric estimation methods. This paper focuses on the traditional approach, although nonparametric techniques are used to justify restrictive assumptions, such as assuming a linear functional form in the production function.


\subsubsection{Production frontier}
Placeholder for Part II.


% 
\section{Data}
<<include=FALSE,eval=TRUE>>=
#Variables----------------------------------------------------------
#COMP =Composite measure of health care attainment, 
#DALE =Disability adjusted life expectancy (other measure), 
#YEAR =1993, . . . , 1997,
#HEXP =Per capita health expenditure, 
#EDUC =Educational attainmen (average years of scooling), 
#COUNTRY=Number assigned to country, 
#OECD =Dummy variable for OECD country (30 countries), 
#GINI =Gini coefficient for income inequality, 
#GEFF =World bank measure of government effectiveness,
#VOICE =World bank measure of democratization of the political process,
#TROPICS =Dummy variable for tropical location, 
#POPDEN =Population density (people per km2) 
#PUBTHE =Proportion of health expenditure paid by public authorities, 
#GDPC =Normalized per capita GDP (in 1997 ppp$). 
#GINI, GEFF, VOICE, POPDEN, PUBTHE an GDPC : only observed for 1997

rm(list = ls())

# Set your the working directory in order to execute locally just chunks of R. Don't forget to outcomment the line once executed
# Your working directory should contain the files "who_data_cc.csv", "who_data_cc_no_irl.csv", "who_data_cc.xlsx" and "empirical project data" until the code will be harmonized

#setwd(" ")  # Jose
#setwd("C:/Users/Nicola/OneDrive/Human Capital/15-17 TSE/AA Semestres/3. Semestre/Empirical project/GitHub Files/EmpProj/Latex code")  # Nicola
#setwd(" ")  # Irina 
#setwd("/Users/custodioij/Dropbox/TSE/M2/Empirical Project/Empirical Project repository/Latex code")  # Igor

#library(pglm)

library(lmtest)
library(sandwich)
library(micEcon)
# library(micEconCES)
library(miscTools)
library(xtable)
library(stargazer)
library(ggplot2)
library(reshape2)

set.seed(123456)
@

<<include=FALSE,eval=TRUE>>=
# Import the data and data cleaning
df <- read.csv("who_data_cc_no_irl.csv", header = TRUE)
#data <- read.xls("who_data_cc.xlsx", sheet = 1, header = TRUE, perl = "C:\\Perl64\\bin\\perl.exe")

# POPDEN is largely wrong and should be used
df2 <- subset(df, COUNTRYNAME != 'Ireland')  # Ireland has COMP=1 in 1993

df2$YEAR <- as.factor(df2$YEAR)
df97 <- subset(df2, YEAR == 1997)
#attach(df97)
#detach(df97)
@

\subsection{The dataset}
The data is collected by the World Health Organization (WHO) and has been used in numerous publications by the WHO, including the World Health Report 2000. Evans \textit{et al.} (2000) and Greene (2004), for example, use this dataset.\footnote{The data is available online at \\ \url{http://pages.stern.nyu.edu/~wgreene/Text/Edition7/tablelist8new.htm}.} The original dataset is an unbalanced panel of 191 countries observed for at most five years. For the purpose of this paper, we used a balanced version of the panel, which consisted of 139 countries and 695 observations. The dataset contains 13 variables, where 4 of them are dummies and 9 are continous variables. With the exception of COMP, DALE, EDUC and HEXP, the other variables are measures for 1997 and therefore do not vary within each country. This lead us to choose the year of 1997 to base any cross-sectional analysis. Table \ref{tab:var.def} summarizes the variables in the dataset.

\begin{table}[htbp] \centering 

  \caption{Variables definition} 
  \label{tab:var.def} 
  \footnotesize
\begin{tabular}{p{3cm} p{9cm}} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
\textbf{Variable name} & \textbf{Definition} \\ 
\hline \\[-1.8ex] 

YEAR & Year from 1993 to 1997 \\
\hline \\[-1.8ex]
COMP & Composite measure of population health. The measure ranges from 0 to 100, where 100 expresses the maximal health care attainment \\
\hline \\[-1.8ex]
DALE & Disability-adjusted life expectancy in years \\
\hline \\[-1.8ex]
HEXP & Health expenditure per capita, measured in 1997 PPP dollars \\
\hline \\[-1.8ex]
EDUC & Average years of schooling \\
\hline \\[-1.8ex]
OECD & Dummy variable for OECD countries (30 countries) \\
\hline \\[-1.8ex]
GINI* & Gini coefficient for income inequality. The coefficient ranges from 0 to 1, where 1 expresses maximal level of inequality \\
\hline \\[-1.8ex]
GEFF* & Government effectiveness, a World Bank measure that ranges from -2 to 2 \\
\hline \\[-1.8ex]
VOICE* & Democratization of the political process. World Bank measure that ranges from -2 to 2 \\
\hline \\[-1.8ex]
TROPICS & Dummy variable for tropical location \\
\hline \\[-1.8ex]
POPDEN* & Population density per square kilometer \\
\hline \\[-1.8ex]
PUBTHE* & Proportion of health expenditure paid by public authorities \\
\hline \\[-1.8ex]
GDPC* & Normalized per capita GDP in 1997 PPP dollars \\
\hline

\hline \\[-1.8ex] 
%\textit{Note:} & \multicolumn{1}{r}{* only observed for 1997} \\ 
&\multicolumn{1}{r}{* only observed for 1997} \\ 
\end{tabular} 
\end{table} 
\normalsize

 
\subsection{Descriptive statistics}
<<setup2, eval=TRUE, echo=F, include=F, cache = F>>=
#setwd("C:/Users/Irina/Desktop/Empirical Project")
library(gdata)
library(pastecs)
library(pbkrtest)
library(plm)
library(corrplot)
data<-read.xls("empirical project data.xlsx", sheet = 1, header = TRUE)
# summary(data)
data<-data[data$COMP!="1",]
data<-na.omit(data)
# summary(data)
df1997 <- subset(data, YEAR == 1997)
# summary(df1997)
df1996 <- subset(data, YEAR == 1996)
# summary(df1996)
df1995 <- subset(data, YEAR == 1995)
# summary(df1995)
df1994 <- subset(data, YEAR == 1994)
# summary(df1994)
df1993 <- subset(data, YEAR == 1993)
# summary(df1993)

plot(c(df1997$COMP,df1997$COMP))
x<-stat.desc(data)
a<-stat.desc(df1993)
b<-stat.desc(df1994)
c<-stat.desc(df1995)
d<-stat.desc(df1996)
e<-stat.desc(df1997)
write.csv(c(x,a,b,c,d,e), "descriptive statistics.csv")
#two-way tables
#twotable<-table(df1997$GDPC., df1997$HEXP)
#prop.table(twotable,1)
#threetable<-xtabs(~df1997$HEXP+df1997$GDPC.+df1997$COMP)
#threetable
#ftable(threetable)
@

Table \ref{tab:desc.stats} shows some descriptive statistics of the dataset. Both candidates for the dependant variable show a similar variation. HEXP and EDUC, however, have largely different ranges. Any coefficient associated to this variables in a regression has to be interpreted accordingly. We can also notice that 20.9\% of our sample belongs to the OECD, and 46.8\% are tropical countries. GINI and GDPC also present large variations, suggesting a high level of development heterogeneity in our sample.

<<desc.stats, results='asis', echo = F>>=
stargazer(df2[, -(1:3)], title = "Descriptive statistics", label = "tab:desc.stats")
@

%First of all, we had to choose our variable of interest on which we will base our analysis.  The two candidates where: DALE which is a measure of life expectancy and COMP which represents a composite variable of health care attainment. By looking at the evolution of these variables across time, we conclude that DALE varies less than COMP. This could be related to the fact that life expectancy is a feature which does not react immediately to a government intervention or to a policy that would affect the health status of citizens and eventually increase their life expectancy. Moreover, the available period in the dataset (5 years) is too short to allow us to conclude about an effect on life expectancy. However, the variable "COMP" is a measure that assess the overall state of healthcare system in a country, and which could be adjusted in  short run as an answer to the manipulation of the main factors of influence. We therefore selected  COMP as the explanatory variable for the further analysis.

In the 1997 cross-sectional setting we split the countries in 3 (slightly balanced) groups depending on their GDP per capita level: low GDP, with a GDP per capita below the threshold of 3000; medium GDP, between 3000 and 7000; and high GDP, when it is greater than 7000.  
The health expenditure for the countries with low GDP lies between 0 and 150\$ per year, while countries with  high GDP level spend on average more than 1000\$ per capita on health care.  Additionally, if we look at the evolution of health expenditure per country, independently of their GDP level, we remark that the average health expenditure increased across time. However, in countries with high GDP the increase was relatively higher (about 11\%) in comparison with countries with low GDP, where the health expenditure has grown by only about 5\% in 5 years.

<<Health expenditure 1997 given GDP, eval=TRUE, echo=F, warning=FALSE, results='asis', fig.cap = "Distribution of HEXP by GDP per capita", out.width='.5\\textwidth', fig.pos = '!htbp', fig.align= "center">>=
tmp <- data.frame(GDP = "Low GDP per capita", HEXP = df1997$HEXP[df1997$GDPC.<3000])
tmp <- rbind(tmp, data.frame(GDP = "Medium GDP per capita",
                             HEXP = df1997$HEXP[df1997$GDPC.>3000 & df1997$GDPC.<7000]))
tmp <- rbind(tmp, data.frame(GDP = "High GDP per capita", HEXP = df1997$HEXP[df1997$GDPC.>7000]))
ggplot(tmp, aes(x = HEXP)) + geom_histogram(bins = 10) +
        facet_grid(. ~ GDP, scales = "free_x") +
        ylab("Frequency") + theme_gray(base_size = 15) #+ theme(aspect.ratio=.5)

# hist(df1997$HEXP[df1997$GDPC.<3000],breaks="FD", main="Health expenditure 1997 for 
#      countries with low GDP",xlab="Health expenditure",col="darkgray")
# hist(df1997$HEXP[df1997$GDPC.>3000 & df1997$GDPC.<7000],breaks="FD", main="Health 
#      expenditure 1997 for countries with average GDP",xlab="Health expenditure",col="darkgray")
# hist(df1997$HEXP[df1997$GDPC.>7000],breaks="FD", main="Health expenditure 1997 for 
#      countries with high GDP",xlab="Health expenditure",col="darkgray")
@

Public contribution is a feature that seems to be highly correlated to the total amount of health expenditure, therefore with the health care attainment measure. In Figure \ref{fig:PUBTHE} shows the behaviour of countries concerning the total heath care attainment, given the proportion of health expenditure paid by public authorities.  There are 55 countries where authorities contribute less than 50\% and 85 countries where authorities contribute more than 50\%. We observe that for the most of countries in which authorities contribute less than 50\% for health expenditure, the health care attainment is between 60 and 80, and for most of countries with a high public contribution, the health care attainment is above 70. We conclude therefore about a positive correlation between these two dimensions.
<<PUBTHE, eval=TRUE, echo=F, results='asis', warning=FALSE,fig.cap = "Distribution of HEXP and COMP by PUPTHE", out.width='.5\\textwidth', fig.pos = '!htbp', fig.align= "center">>=
tmp <- data.frame(PUBTHE = "Low public contribution",
        value = c(df1997$COMP[df1997$PUBTHE<50], df1997$HEXP[df1997$PUBTHE<50]),
        measure = c(rep('COMP', sum(df1997$PUBTHE<50)),
                    rep('HEXP', sum(df1997$PUBTHE<50))))
tmp <- rbind(tmp, data.frame(PUBTHE = "High public contribution",
                        value = c(df1997$COMP[df1997$PUBTHE>50],
                                df1997$HEXP[df1997$PUBTHE>50]),
                        measure = c(rep('COMP', sum(df1997$PUBTHE>50)),
                                rep('HEXP', sum(df1997$PUBTHE>50)))))
ggplot(tmp, aes(x = value)) + geom_histogram(bins = 10) +
        facet_grid(PUBTHE ~ measure, scales = "free") +
        ylab("Frequency") + theme_gray(base_size = 15) + xlab('Value')# + coord_fixed(ratio=100)

# hist(df1997$COMP[df1997$PUBTHE<50],breaks="FD", main="Health care attainment for 
#      countries with low public contribution",xlab="Health care attainment",col="darkgray")
# hist(df1997$COMP[df1997$PUBTHE>50],breaks="FD", main="Health care attainment for 
#      countries with high public contribution",xlab="Health care attainment",col="darkgray")
@

%Based on the axiom that  rationality, that we can easily associate to the years of schooling, influence people to make educated decisions in terms of personal health status; we expect countries which have a population with a higher education level to have higher health expenditure. However, according to Andrew Balls' article in the National Bureau of Economic Research, in most of the cases, tropical countries are considered as "underdeveloped countries" ; we have to take into account, therefore,  that the education level in tropical countries and in temperate  countries is different. Moreover, in tropical countries there are additional factors that have a negative influence on the population's health status, as the higher temperature leads to an accelerated propagation of bacteria.  It would be interesting, therefore, to see how the level of education impacts the health expenditure by controllng for the geographical zone.

Figure \ref{fig:OECD} shows how the OECD variable aggregates most of the developed countries in our sample.

<<OECD, cache =T, eval=T, echo=F, results='asis',,fig.cap = "EDUC and HEXP for according to TROPICAL", out.width='.5\\textwidth', fig.pos = '!htbp', fig.align= "center">>=
tmp <- data.frame(EDUC = data$EDUC, COMP = data$COMP, HEXP = data$HEXP, 
                  OECD = (data$OECD == 1))[-701,]
tmp <- melt(tmp, id = c('COMP', 'OECD'))
ggplot(tmp, aes(x = value, y = COMP, colour = OECD)) + geom_point() +
        theme_gray(base_size = 15) + facet_grid(.~ variable, scales = "free") +
        xlab('Value') + theme(legend.position = "bottom") #+ coord_fixed(ratio=50)
@

%\subsection{Cluster analysis}

<<eval=F,include=F>>=
N <- length(prod.CD.contr.pooled$residuals)
p <- length(prod.CD.contr.pooled$coefficients)
sigma <- sqrt(var(prod.CD.contr.pooled$residuals)) * sqrt((N-p)/N)
sum(dnorm(comp, mean=fitted(prod.CD.contr.pooled), sd=sigma, log=TRUE))
sum(dnorm(resid(prod.CD.contr.pooled), mean=0, sd=sigma, log=TRUE))
@

%---------------------------------------------------------------------------
%
%--------------- Part I
%
%---------------------------------------------------------------------------

\section{Model specification}
\subsection{Nonparametric analysis of the dataset}
The parametric models we considered for the production function rely on linear functional forms. Therefore, in this section, we develop evidence from the data to support the restrictions imposed on the functional form of the regression models used later on. We inspect in a nonparametric setting the two candidates for dependent variable, COMP and DALE, along with the two main inputs of interest, EDUC and HEXP. Using a conditional mean regression model and by comparing the local constant estimator and local linear estimator againts the OLS estimator, we find evidence for a linear functional form in our production function. 
 
Consider the following conditional mean regression model for the production function of a given country $i$'s health care system

$$
y_i = g(x_i) + \epsilon_i \quad \textrm{and} \quad \mathbb{E}[\epsilon|x]=0 \quad \forall i
$$
then
$$
\mathbb{E}[y|x]=g(x_i)
$$

where $y$ is a measure of national health, $x$ is an input of interest (or a vector of inputs of interest), and $g$ is the nonparametric function we wish to estimate.
 
 %For simplicity in nomenclature, we define lowercase variable names as the logs of the original variables -- for example: dale=log(DALE). 
All analysis is performed on the log of the variables, not on the levels. We use a second-order Gaussian Kernel as the weighting factor. Optimal bandwiths are chosen using cross validation and then undersmoothed. All figures show the OLS estimator, the local constant estimator (or Nadaraya-Watson) estimator (LC), and the local linear estimator (LL). The analysis is first performed on the cross section of 1997 and then on all the years in the panel.

<<Data, eval=TRUE, echo=FALSE, cache=TRUE, include=FALSE>>=
library(gdata)
library(np)
library(MASS)

data <- read.csv("who_data_cc_no_irl.csv", header = TRUE)
data <- data[1:695,] 
data1997 <- subset(data, YEAR == 1997)

data1997$logcomp97 <- log(data1997$COMP) #15 (org. #3)
data1997$logdale97 <- log(data1997$DALE) #16 (org. #4)
data1997$logeduc97 <- log(data1997$EDUC) #17 (org. #5)
data1997$loghexp97 <- log(data1997$HEXP) #18 (org. #6)

data$logcomp <- log(data$COMP) #15 (org. #3)
data$logdale <- log(data$DALE) #16 (org. #4)
data$logeduc <- log(data$EDUC) #17 (org. #5)
data$loghexp <- log(data$HEXP) #18 (org. #6)

@

<<Functions, eval=TRUE, echo=FALSE, cache=TRUE>>=

## Defining the kernels:
ker <- function(v,kern){
       if (kern=="Ep") {
             K<-0.75*(1-v^2)*(abs(v)<=1) 
       }
       if (kern=="Ga"){
             K<-1/(sqrt(2*pi))*exp(-0.5*v^2)
       }
       return(K)
}

## Sum of kernels:
sumkern <- function(x,x0,h,kern){
       S<-0
       for (i in seq(1,length(x),by=1)){
             arg<-(x[i]-x0)/h
            S<-S+ker(arg,kern)
      }
       return(S)
}

## Numerator of NW estimator
sumkerY <- function(X,Y,x,h,kern){
      Sy<-0
       for (i in seq(1,length(X),by=1)){
             arg<-(X[i]-x)/h
             Sy<-Sy+(ker(arg,kern)*Y[i])
       }
      return(Sy)
}

## Nadaraya-Watson or a Local Constant Estimator
NW <- function(X,Y,x,h,kern){
       g=sumkerY(X,Y,x,h,kern)/sumkern(X,x,h,kern)	
       return(g)	
}
 
## Local Polynomial Estimator of degree p
LL <- function(X,Y,x,h,kern,p){
      nn=length(X)
      e=rep(1,nn)
      X.minus.x=X-e*x
      
       Z=matrix(1,nn,p+1)
       
       if(p==1){
             Z = cbind(e,X.minus.x)
       }
       if(p==2){
             Z = cbind(e,X.minus.x,(X.minus.x)^2)
       }
       
       denom =matrix(0,p+1,p+1)
       for (i in seq(1,nn,by=1)){
             arg=(X[i]-x)/h
             denom=denom+ker(arg,kern)*Z[i,]%*% t(Z[i,])  ## %*% is for matrix multiplication
       }
       denom.inv=ginv(denom)
 
       numer=matrix(0,p+1,1)
       for (i in seq(1,nn,by=1)){
             arg=(X[i]-x)/h
             numer=numer+ker(arg,kern)*Z[i,]*Y[i]
       }
 
       ghat=denom.inv%*% numer
       g=ghat[1]
       return(g)	
}
@
 
\subsubsection{Cross section for 1997}
 
We first look at each possible combination between the dependent variables and independent variables (i.e. univariate analysis). As seen in Figure \ref{fig:np.cs.uni.plot}, the LL and LC estimators mimic -- and sometimes overlap -- the OLS estimator. Simimilarly, we notice that each combination presents an upward trend.
 
When looking at both inputs simultaneously on each dependent vartiable (i.e. multivariate analysis, Figure \ref{fig:np.cs.multi}), we find upwarding planes, presenting a spike at low levels of educational attainment with high levels of per capita health expenditure. On one hand, from these figures, it appears that a country with low levels of education will have to spend a greater quantity on health to achieve a better health care system. On the other hand, it appears that a country with low health expenditure can still achieve a decent health care system as long as the population has a certain educational level.

<<np.cs.uni, logcomp, logeduc, eval=TRUE, echo=FALSE, cache=TRUE, results='hide', include=TRUE>>=
# par(mfrow=c(2,2))

n <- nrow(data1997)
Y <- data1997[,16]
X <- data1997[,18]
z <- seq(min(X), max(X), by=0.05)

## OLS
tilde.X=cbind(rep(1,n),X)
beta.hat=ginv(t(tilde.X)%*% tilde.X)%*% t(tilde.X)%*% Y
# beta.hat
g.OLS.1=beta.hat[1]+beta.hat[2]*z

## Nonparametric estimation
temp = npregbw(X,Y,regtype="lc", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.lc = temp[['bw']] 

temp = npregbw(X,Y,regtype="ll", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.ll = temp[['bw']] 

h.tol = 10^(-3) #undersmoothing optimal bandwith

g.LC.1 = matrix(0,length(z),1)
g.LL.1 = matrix(0,length(z),1)

for (i in seq(1,length(z),by=1)){
      
       g.LC.1[i]<- NW(X,Y,z[i],h.lc-h.tol,"Ga")	
       g.LL.1[i]<- LL(X,Y,z[i],h.ll-h.tol,"Ga",p=1)	 
       
}

tmp.plot1 <- data.frame(x = z, y = g.OLS.1, type = 'OLS')
tmp.plot1 <- rbind.data.frame(tmp.plot1, data.frame(
                x = data1997$logeduc97, y = data1997$logcomp97, type = 'points'),
        data.frame(
                x = z, y = g.LC.1, type = 'LC'),
        data.frame(
                x = z, y = g.LL.1, type = 'LL')
)


# plot(z,g.OLS.1,type="l",col=c("black"),lty=c(1), xlab = "educ 1997", ylab = "comp 1997",ylim=c(3.8,4.6), main="comp regressed on educ, 1997")
# points(data1997$logeduc97, data1997$logcomp97, type = "p")
# lines(z,g.LC.1,col=c("blue"),lty=c(1))
# lines(z,g.LL.1,col=c("red"),lty=c(1))
# legend("topleft", c("OLS","LC", "LL"), col = c("black","blue","red"), lty = c(1,1,1), cex = 0.75)
 

n <- nrow(data1997)
Y <- data1997[,17]
X <- data1997[,18]
z <- seq(min(X), max(X), by=0.05)

## OLS
tilde.X=cbind(rep(1,n),X)
beta.hat=ginv(t(tilde.X)%*% tilde.X)%*% t(tilde.X)%*% Y
g.OLS.3=beta.hat[1]+beta.hat[2]*z

## Nonparametric estimation
temp = npregbw(X,Y,regtype="lc", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.lc = temp[['bw']] 

temp = npregbw(X,Y,regtype="ll", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.ll = temp[['bw']] 

h.tol = 10^(-3) #undersmoothing optimal bandwith

g.LC.3 = matrix(0,length(z),1)
g.LL.3 = matrix(0,length(z),1)
 
for (i in seq(1,length(z),by=1)){
      
       g.LC.3[i]<- NW(X,Y,z[i],h.lc-h.tol,"Ga")	
       g.LL.3[i]<- LL(X,Y,z[i],h.ll-h.tol,"Ga",p=2)	 
       
}

tmp.plot3 <- data.frame(x = z, y = g.OLS.3, type = 'OLS')
tmp.plot3 <- rbind.data.frame(tmp.plot3, data.frame(
                x = data1997$logeduc97, y = data1997$logdale97, type = 'points'),
        data.frame(
                x = z, y = g.LC.3, type = 'LC'),
        data.frame(
                x = z, y = g.LL.3, type = 'LL')
)

# plot(z,g.OLS.3,type="l",col=c("black"),lty=c(1), xlab = "educ 1997", ylab = "dale 1997", ylim=c(3.3,4.5), main="dale regressed on educ, 1997")
# points(data1997$logeduc97, data1997$logdale97, type = "p")
# lines(z,g.LC.3,col=c("blue"),lty=c(1))
# lines(z,g.LL.3,col=c("red"),lty=c(1))



n <- nrow(data1997)
Y <- data1997[,16]
X <- data1997[,19]
z <- seq(min(X), max(X), by=0.05)
 
## OLS
tilde.X=cbind(rep(1,n),X)
beta.hat=ginv(t(tilde.X)%*% tilde.X)%*% t(tilde.X)%*% Y
g.OLS.2=beta.hat[1]+beta.hat[2]*z

## Nonparametric estimation
temp = npregbw(X,Y,regtype="lc", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.lc = temp[['bw']] 

temp = npregbw(X,Y,regtype="ll", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.ll = temp[['bw']] 

h.tol = 10^(-3) #undersmoothing optimal bandwith

g.LC.2 = matrix(0,length(z),1)
g.LL.2 = matrix(0,length(z),1)

for (i in seq(1,length(z),by=1)){
      
       g.LC.2[i]<- NW(X,Y,z[i],h.lc-h.tol,"Ga")	
       g.LL.2[i]<- LL(X,Y,z[i],h.ll-h.tol,"Ga",p=1)	 
       
}

tmp.plot2 <- data.frame(x = z, y = g.OLS.2, type = 'OLS')
tmp.plot2 <- rbind.data.frame(tmp.plot2, data.frame(
                x = data1997$loghexp97, y = data1997$logcomp97, type = 'points'),
        data.frame(
                x = z, y = g.LC.2, type = 'LC'),
        data.frame(
                x = z, y = g.LL.2, type = 'LL')
)

# plot(z,g.OLS.2,type="l",col=c("black"),lty=c(1), xlab = "hexp 1997", ylab = "comp 1997", ylim=c(3.8,4.6), main="comp regressed on hexp, 1997")
# points(data1997$loghexp97, data1997$logcomp97, type = "p")
# lines(z,g.LC.2,col=c("blue"),lty=c(1))
# lines(z,g.LL.2,col=c("red"),lty=c(1))
 
n <- nrow(data1997)
Y <- data1997[,17]
X <- data1997[,19]
z <- seq(min(X), max(X), by=0.05)
 
## OLS
tilde.X=cbind(rep(1,n),X)
beta.hat=ginv(t(tilde.X)%*% tilde.X)%*% t(tilde.X)%*% Y
g.OLS.4=beta.hat[1]+beta.hat[2]*z

## Nonparametric estimation
temp = npregbw(X,Y,regtype="lc", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.lc = temp[['bw']] 

temp = npregbw(X,Y,regtype="ll", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.ll = temp[['bw']] 

h.tol = 10^(-3) #undersmoothing optimal bandwith

g.LC.4 = matrix(0,length(z),1)
g.LL.4 = matrix(0,length(z),1)

for (i in seq(1,length(z),by=1)){
      g.LC.4[i]<- NW(X,Y,z[i],h.lc-h.tol,"Ga")	
      g.LL.4[i]<- LL(X,Y,z[i],h.ll-h.tol,"Ga",p=1)	 
      
}

tmp.plot4 <- data.frame(x = z, y = g.OLS.4, type = 'OLS')
tmp.plot4 <- rbind.data.frame(tmp.plot4, data.frame(
                x = data1997$loghexp97, y = data1997$logdale97, type = 'points'),
        data.frame(
                x = z, y = g.LC.4, type = 'LC'),
        data.frame(
                x = z, y = g.LL.4, type = 'LL')
)

# plot(z,g.OLS.4,type="l",col=c("black"),lty=c(1), xlab = "hexp 1997", ylab = "dale 1997", ylim=c(3.3,4.5), main="dale regressed on hexp, 1997")
# points(data1997$loghexp97, data1997$logdale97, type = "p")
# lines(z,g.LC.4,col=c("blue"),lty=c(1))
# lines(z,g.LL.4,col=c("red"),lty=c(1))
tmp.plot1 <- cbind(tmp.plot1, xvar = 'EDUC', yvar = 'COMP')
tmp.plot3 <- cbind(tmp.plot3, xvar = 'EDUC', yvar = 'DALE')
tmp.plot2 <- cbind(tmp.plot2, xvar = 'HEXP', yvar = 'COMP')
tmp.plot4 <- cbind(tmp.plot4, xvar = 'HEXP', yvar = 'DALE')
df.np.cs.uni <- rbind(tmp.plot1, tmp.plot3, tmp.plot2, tmp.plot4)
@

<<np.cs.uni.plot, eval=TRUE, echo=FALSE, cache=TRUE, results='hide', include=TRUE, fig.cap='Univariate nonparametric analysis for 1997',fig.align='center',out.width='0.9\\textwidth',fig.pos='htbp'>>=
ggplot(df.np.cs.uni, aes(x = x, y = y, linetype = type)) +
        facet_grid(yvar ~ xvar, scales = 'free') +
        geom_line(data = subset(df.np.cs.uni, type != 'points')) +
        geom_point(data = subset(df.np.cs.uni, type == 'points'), alpha = .5) +
        scale_linetype_discrete(name = "Title", labels = c("OLS", "LC", "LL"),
                                limits = c("OLS", "LC", "LL"))
@

<<np.cs.multi, eval=TRUE, echo=FALSE, cache=TRUE, results='hide', include=TRUE, fig.cap='Multivariate nonparametric analysis for 1997',fig.align='center',out.width='\\textwidth',fig.pos='htbp'>>=
par(mfrow=c(1,2))

mod_comp_OLS <- lm(data1997$logcomp97 ~ data1997$logeduc97 + data1997$loghexp97)
# summary(mod_comp_OLS)

comp_1997=data1997$logcomp97
dale_1997=data1997$logdale97
educ_1997=data1997$logeduc97
hexp_97=data1997$loghexp97

mod_comp_ll <- npreg(data1997$logcomp97 ~ data1997$logeduc97 + data1997$loghexp97, regtype = "ll")
mod_comp_ll_2 <- npreg(comp_1997 ~ educ_1997 + hexp_97, regtype = "ll")
# summary(mod_comp_ll)
plot(mod_comp_ll_2, view = "fixed", xlab = "educ 1997", ylab= "hexp 97", zlab = "comp 1997", main="LL regression, comp 1997")

mod_dale_OLS <- lm(data1997$logdale97 ~ data1997$logeduc97 + data1997$loghexp97)
summary(mod_dale_OLS)

mod_dale_ll <- npreg(data1997$logdale97 ~ data1997$logeduc97 + data1997$loghexp97, regtype = "ll")
mod_dale_ll_2 <- npreg(dale_1997 ~ educ_1997 + hexp_97, regtype = "ll")
summary(mod_dale_ll)
plot(mod_dale_ll_2, view = "fixed", xlab = "educ 1997", ylab= "hexp 97", zlab = "dale 1997", main="LL regression, dale 1997")
@

\subsubsection{Pooled data from 1993 to 1997}

As in the case for only 1997, when looking at the pooled data in the univariate case, all figures present an upward trend. The LL and LC estimators follow the OLS estimator, except in those parts where the lack of observations alters the smoothness of the lines (but this is a consequence of the bandwith). This reinforces the assumption of linear functional form of the production function. Worth mentioning is how little countries' DALE and COMP change over time. Notice that each figure presents clusters of consecutive points, which represent a given country over the five-year period. This implies that there is little variation within the country. The source of variation should derive from between countries. Similarly, the greatest amount of variation between countries overtime appears at lower levels of HEXP and EDUC, respectively. It appears that poorer countries' health care systems vary more while richer countries have similar outcomes. The cluster of points for poorer countries are more spread out that for the richer countries, which implies that a higher health expenditure/educational attainmnent overtime have a a greater impact on the given dependent variable and which could point out to economies of scale. We do not, however, present the graphs for the multivariate case. Planes were obtained, but results were not insightful due to the large variation in bandwiths.

The nonparametric analysis sugests that a linear functional form on the logs of EDUC and HEXP are an adequate model for COMP and DALE.
<<all, eval=TRUE, echo=FALSE, cache=TRUE, results='hide', include=TRUE>>=
par(mfrow=c(2,2))

n <- nrow(data)
Y <- data[,16]
X <- data[,18]
z <- seq(min(X), max(X), by=0.05)

## OLS
tilde.X=cbind(rep(1,n),X)
beta.hat=ginv(t(tilde.X)%*% tilde.X)%*% t(tilde.X)%*% Y
g.OLS=beta.hat[1]+beta.hat[2]*z

## Nonparametric estimation
temp = npregbw(X,Y,regtype="lc", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.lc = temp[['bw']] 

temp = npregbw(X,Y,regtype="ll", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.ll = temp[['bw']] 

h.tol = 10^(-3) #undersmoothing optimal bandwith

g.LC = matrix(0,length(z),1)
g.LL = matrix(0,length(z),1)

for (i in seq(1,length(z),by=1)){
      
      g.LC[i]<- NW(X,Y,z[i],h.lc-h.tol,"Ga")	
      g.LL[i]<- LL(X,Y,z[i],h.ll-h.tol,"Ga",p=1)	 
      
}

tmp.plot1 <- data.frame(x = z, y = g.OLS, type = 'OLS')
tmp.plot1 <- rbind.data.frame(tmp.plot1, data.frame(
                x = data1997$logeduc97, y = data1997$logcomp97, type = 'points97'),
        data.frame(
                x = data$logeduc, y = data$logcomp, type = 'points'),
        data.frame(
                x = z, y = g.LC, type = 'LC'),
        data.frame(
                x = z, y = g.LL, type = 'LL')
)
# plot(z,g.OLS,type="l",col=c("black"),lty=c(1), xlab = "educ", ylab = "comp", ylim=c(3.8,4.6), main="comp regr. on educ, pooled")
# points(data$logeduc, data$logcomp, type = "p", col=c("gray"))
# points(data1997$logeduc97, data1997$logcomp97, type = "p", col=c("black"))
# lines(z,g.LC,col=c("blue"),lty=c(1))
# lines(z,g.LL,col=c("red"),lty=c(1))
# legend("topleft", c("OLS","LC", "LL", "1997"), col = c("black","blue","red","black"), lty = c(1,1,1,NA), pch=c(NA,NA,NA,1), cex = 0.75)



n <- nrow(data)
Y <- data[,17]
X <- data[,18]
z <- seq(min(X), max(X), by=0.05)

## OLS
tilde.X=cbind(rep(1,n),X)
beta.hat=ginv(t(tilde.X)%*% tilde.X)%*% t(tilde.X)%*% Y
g.OLS=beta.hat[1]+beta.hat[2]*z

## Nonparametric estimation
temp = npregbw(X,Y,regtype="lc", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.lc = temp[['bw']] 

temp = npregbw(X,Y,regtype="ll", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.ll = temp[['bw']] 

h.tol = 10^(-3) #undersmoothing optimal bandwith

g.LC = matrix(0,length(z),1)
g.LL = matrix(0,length(z),1)

for (i in seq(1,length(z),by=1)){
      
      g.LC[i]<- NW(X,Y,z[i],h.lc-h.tol,"Ga")	
      g.LL[i]<- LL(X,Y,z[i],h.ll-h.tol,"Ga",p=2)	 
      
}
tmp.plot2 <- data.frame(x = z, y = g.OLS, type = 'OLS')
tmp.plot2 <- rbind.data.frame(tmp.plot2, data.frame(
                x = data1997$logeduc97, y = data1997$logdale97, type = 'points97'),
        data.frame(
                x = data$logeduc, y = data$logdale, type = 'points'),
        data.frame(
                x = z, y = g.LC, type = 'LC'),
        data.frame(
                x = z, y = g.LL, type = 'LL')
)
# plot(z,g.OLS,type="l",col=c("black"),lty=c(1), xlab = "educ", ylab = "dale", ylim=c(3.3,4.5), main="dale regr. on educ, pooled")
# points(data$logeduc, data$logdale, type = "p", col=c("gray"))
# points(data1997$logeduc97, data1997$logdale97, type = "p", col=c("black"))
# lines(z,g.LC,col=c("blue"),lty=c(1))
# lines(z,g.LL,col=c("red"),lty=c(1))



n <- nrow(data)
Y <- data[,16]
X <- data[,19]
z <- seq(min(X), max(X), by=0.05)

## OLS
tilde.X=cbind(rep(1,n),X)
beta.hat=ginv(t(tilde.X)%*% tilde.X)%*% t(tilde.X)%*% Y
g.OLS=beta.hat[1]+beta.hat[2]*z

## Nonparametric estimation
temp = npregbw(X,Y,regtype="lc", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.lc = temp[['bw']] 

temp = npregbw(X,Y,regtype="ll", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.ll = temp[['bw']] 

h.tol = 10^(-3) #undersmoothing optimal bandwith

g.LC = matrix(0,length(z),1)
g.LL = matrix(0,length(z),1)

for (i in seq(1,length(z),by=1)){
      
      g.LC[i]<- NW(X,Y,z[i],h.lc-h.tol,"Ga")	
      g.LL[i]<- LL(X,Y,z[i],h.ll-h.tol,"Ga",p=1)	 
      
}
tmp.plot3 <- data.frame(x = z, y = g.OLS, type = 'OLS')
tmp.plot3 <- rbind.data.frame(tmp.plot3, data.frame(
                x = data1997$loghexp97, y = data1997$logcomp97, type = 'points97'),
        data.frame(
                x = data$loghexp, y = data$logcomp, type = 'points'),
        data.frame(
                x = z, y = g.LC, type = 'LC'),
        data.frame(
                x = z, y = g.LL, type = 'LL')
)
# plot(z,g.OLS,type="l",col=c("black"),lty=c(1), xlab = "hexp", ylab = "comp", ylim=c(3.8,4.6), main="comp regr. on hexp, pooled")
# points(data$loghexp, data$logcomp, type = "p", col=c("gray"))
# points(data1997$loghexp97, data1997$logcomp97, type = "p", col=c("black"))
# lines(z,g.LC,col=c("blue"),lty=c(1))
# lines(z,g.LL,col=c("red"),lty=c(1))



n <- nrow(data)
Y <- data[,17]
X <- data[,19]
z <- seq(min(X), max(X), by=0.05)

## OLS
tilde.X=cbind(rep(1,n),X)
beta.hat=ginv(t(tilde.X)%*% tilde.X)%*% t(tilde.X)%*% Y
g.OLS=beta.hat[1]+beta.hat[2]*z

## Nonparametric estimation
temp = npregbw(X,Y,regtype="lc", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.lc = temp[['bw']] 

temp = npregbw(X,Y,regtype="ll", bwmethod="cv.ls",bwtype="fixed", ckertype="gaussian", ckerorder=2)
h.ll = temp[['bw']] 

h.tol = 10^(-3) #undersmoothing optimal bandwith

g.LC = matrix(0,length(z),1)
g.LL = matrix(0,length(z),1)

for (i in seq(1,length(z),by=1)){
      
      g.LC[i]<- NW(X,Y,z[i],h.lc-h.tol,"Ga")	
      g.LL[i]<- LL(X,Y,z[i],h.ll-h.tol,"Ga",p=1)	 
      
}
tmp.plot4 <- data.frame(x = z, y = g.OLS, type = 'OLS')
tmp.plot4 <- rbind.data.frame(tmp.plot4, data.frame(
                x = data1997$loghexp97, y = data1997$logdale97, type = 'points97'),
        data.frame(
                x = data$loghexp, y = data$logdale, type = 'points'),
        data.frame(
                x = z, y = g.LC, type = 'LC'),
        data.frame(
                x = z, y = g.LL, type = 'LL')
)
# plot(z,g.OLS,type="l",col=c("black"),lty=c(1), xlab = "hexp", ylab = "dale", ylim=c(3.3,4.5), main="dale regr. on hexp, pooled")
# points(data$loghexp, data$logdale, type = "p", col=c("gray"))
# points(data1997$loghexp97, data1997$logdale97, type = "p", col=c("black"))
# lines(z,g.LC,col=c("blue"),lty=c(1))
# lines(z,g.LL,col=c("red"),lty=c(1))
tmp.plot1 <- cbind(tmp.plot1, xvar = 'EDUC', yvar = 'COMP')
tmp.plot2 <- cbind(tmp.plot2, xvar = 'EDUC', yvar = 'DALE')
tmp.plot3 <- cbind(tmp.plot3, xvar = 'HEXP', yvar = 'COMP')
tmp.plot4 <- cbind(tmp.plot4, xvar = 'HEXP', yvar = 'DALE')
df.np.pooled.uni <- rbind(tmp.plot1, tmp.plot2, tmp.plot3, tmp.plot4)
@

<<np.pooled.uni.plot, eval=TRUE, echo=FALSE, cache=TRUE, results='hide', include=TRUE, fig.cap='Univariate nonparametric analysis for pooled data',fig.align='center',out.width='0.9\\textwidth',fig.pos='htbp'>>=
ggplot(df.np.pooled.uni, aes(x = x, y = y, linetype = type)) +
        facet_grid(yvar ~ xvar, scales = 'free') +
        geom_line(data = subset(df.np.pooled.uni, (type != 'points')&(type != 'points97'))) +
        geom_point(data = subset(df.np.pooled.uni, (type == 'points')),
                   alpha = .2, colour = "black") +
        geom_point(data = subset(df.np.pooled.uni, (type == 'points97')),
                   alpha = .5, colour = "red") +
        scale_linetype_discrete(name = "Title", labels = c("OLS", "LC", "LL"),
                                limits = c("OLS", "LC", "LL"))
@

<<include=FALSE,eval=TRUE>>=
attach(df97)
#detach(df97)
comp <- log(COMP)
dale <- log(DALE)
hexp <- log(HEXP)
educ <- log(EDUC)
gini <- log(GINI)
popden <- log(POPDEN)
gdpc <- log(GDPC)

# not used ("COUNTRY", "YEAR", "OECD", "GEFF", "VOICE", "TROPICS", "PUBTHE")
GEFF_2 <- GEFF + 2
geff <- log(GEFF_2)
VOICE_2 <- VOICE + 2
voice <- log(VOICE_2)
pubthe <- log(PUBTHE)
@

\subsection{Choise of the output variable}
The dataset provides two measures of population health: DALE (disability-adjusted life expectancy) and COMP (composite meansure of population health). DALE is a measure of life expectancy, adjusted downward to take into account the time lived with a disability. COMP is a measure composed by three elements: the first element is the disability-adjusted life expectancy (the DALE measure), the second element is a measure for resposiveness\footnote{Responsiveness measures "how the system performs relative to non-health aspects" and whether it meets or not "a population's expectations of how it should be treated by providers of prevention and care". How the system responds to health needs already shows up in the DALE measure.} and the third element is a measure for fair financing\footnote{Fair financing measures whether "the risks each household faces due to the costs of the health system are distributed according to ability to pay rather than to the risk of illness".}. These measure are calcualted by the World Health Organization and their elements are weighted according to the importance attributed to each of them on average. The weights are 50\%, 25\% and 25\% respectively. \\

A possible criterion for the choice of the output variable consists in chosing the variable with larger variation both within and between countries. An overview of the variation over time of the two output variables is given in Figure \ref{fig:boxplot_DALE_COMP} in the Appendix. Table \ref{Within variance of DALE and COMP} and Table \ref{Between variance of DALE and COMP} show the within and between variance of the two output variables. The within variance has been computed for each country over the observed five years. Consequently, mean and median have been built. The between variance is displayed for each of the five years. DALE has a larger variation over time and over countries. However, the difference in comparison with the variance of COMP seems to be small. We select COMP as the dependent output variable.

<<include=FALSE,eval=TRUE>>=
var.within.DALE <- c()
for (i in 1:140) {
  var.within.DALE[i] <- var(subset(df2,COUNTRYCODE==COUNTRYCODE[i])$DALE)
}
# mean(var.within.DALE)
# median(var.within.DALE)

var.within.COMP <- c()
for (i in 1:140) {
  var.within.COMP[i] <- var(subset(df2,COUNTRYCODE==COUNTRYCODE[i])$COMP)
}
# mean(var.within.COMP)
# median(var.within.COMP)
# 
# 
# var(subset(df2,YEAR==1993)$DALE)
# var(subset(df2,YEAR==1994)$DALE)
# var(subset(df2,YEAR==1995)$DALE)
# var(subset(df2,YEAR==1996)$DALE)
# var(subset(df2,YEAR==1997)$DALE)
# 
# var(subset(df2,YEAR==1993)$COMP)
# var(subset(df2,YEAR==1994)$COMP)
# var(subset(df2,YEAR==1995)$COMP)
# var(subset(df2,YEAR==1996)$COMP)
# var(subset(df2,YEAR==1997)$COMP)
@

\begin{table}[htbp] \centering 
  \caption{Within variance} 
  \label{Within variance of DALE and COMP} 
  \normalsize
\begin{tabular}{p{5cm} p{1.5cm} p{1.5cm}} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & Mean & Median \\ 
\hline \\[-1.8ex] 
DALE & \Sexpr{round(mean(var.within.DALE),2)} & \Sexpr{round(median(var.within.DALE),2)} \\
\hline \\[-1.8ex] 
COMP & \Sexpr{round(mean(var.within.COMP),2)} & \Sexpr{round(median(var.within.COMP),2)} \\
\hline
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

\begin{table}[htbp] \centering 
  \caption{Between variance} 
  \label{Between variance of DALE and COMP} 
  \normalsize
\begin{tabular}{p{5cm} p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm} p{1.2cm}} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & 1993 & 1994 & 1995 & 1996 & 1997 \\ 
\hline \\[-1.8ex] 
DALE & \Sexpr{round(var(subset(df2,YEAR==1993)$DALE),2)} & \Sexpr{round(var(subset(df2,YEAR==1994)$DALE),2)} & \Sexpr{round(var(subset(df2,YEAR==1995)$DALE),2)} & \Sexpr{round(var(subset(df2,YEAR==1996)$DALE),2)} & \Sexpr{round(var(subset(df2,YEAR==1997)$DALE),2)} \\
\hline \\[-1.8ex] 
COMP & \Sexpr{round(var(subset(df2,YEAR==1993)$COMP),2)} & \Sexpr{round(var(subset(df2,YEAR==1994)$COMP),2)} & \Sexpr{round(var(subset(df2,YEAR==1995)$COMP),2)} & \Sexpr{round(var(subset(df2,YEAR==1996)$COMP),2)} & \Sexpr{round(var(subset(df2,YEAR==1997)$COMP),2)} \\
\hline
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}



<<include=FALSE,eval=TRUE>>=
# Cobb-Douglas
prod.CD <- lm(comp ~ hexp + educ, data = df97)
summary(prod.CD)
# Cobb-Douglas plus controls
prod.CD.contr <- lm(comp ~ hexp + educ 
                    + gini + I(gini^2) + popden + VOICE +  OECD, data = df97)
summary(prod.CD.contr)

# Translog
prod.TL <- lm(comp ~ hexp + educ 
              + I(hexp^2) + I(educ^2) 
              + I(hexp*educ), data = df97)
summary(prod.TL)
# Translog plus controls
prod.TL.contr <- lm(comp ~ hexp + educ 
                    + I(hexp^2) + I(educ^2) 
                    + I(hexp*educ)
                    + gini + I(gini^2) + popden + VOICE +  OECD, data = df97)
summary(prod.TL.contr)

# Truncated Translog
prod.TTL <- lm(comp ~ hexp + educ
               + I(educ^2), data = df97)
summary(prod.TTL)
# Truncated Translog plus controls
prod.TTL.contr <- lm(comp ~ hexp + educ
                     + I(educ^2)
                     + gini + I(gini^2) + popden + VOICE +  OECD, data = df97)
summary(prod.TTL.contr)

# Wald and LR tests
WT.CDvTL <- waldtest(prod.CD, prod.TL)   # no significant difference -> CD
WT.CDvTTL <- waldtest(prod.CD, prod.TTL)  # no significant difference -> CD
WT.TTLvTL <- waldtest(prod.TTL, prod.TL)  # no significant difference -> TTL
LR.CDvTL <- lrtest(prod.CD, prod.TL)   # no significant difference -> CD
LR.CDvTTL <- lrtest(prod.CD, prod.TTL)  # no significant difference -> CD
LR.TTLvTL <- lrtest(prod.TTL, prod.TL)  # no significant difference -> TTL

WT.CDvTL.contr <- waldtest(prod.CD.contr, prod.TL.contr)   # significant difference -> TL (same result if we drop POPDEN)
WT.CDvTTL.contr <- waldtest(prod.CD.contr, prod.TTL.contr)  # no significant difference -> CD
WT.TTLvTL.contr <- waldtest(prod.TTL.contr, prod.TL.contr)  # significant difference -> TL
LR.CDvTL.contr <- lrtest(prod.CD.contr, prod.TL.contr)   # significant difference -> TL (same result if we drop POPDEN)
LR.CDvTTL.contr <- lrtest(prod.CD.contr, prod.TTL.contr)  # no significant difference -> CD
LR.TTLvTL.contr <- lrtest(prod.TTL.contr, prod.TL.contr)  # significant difference -> TL

# Constant elasticities of substitution
#c("HEXP", "EDUC")
#prod.CES <- 
#summary(prod.CES)
@




\subsection{Functional form}
The Cobb-Douglas production function has the following form:
\begin{gather}
COMP_i = HEXP_i^{\beta_1}EDUC_i^{\beta_2}
\end{gather}
After taking the log on both sides the equation is equivalent to
\begin{gather}
comp_i = \beta_1 hexp_i + \beta_2 educ_i
\end{gather}
which can be estimated by an OLS model of the form:
\begin{gather}
comp_i = \beta_0 + \beta_1 hexp_i + \beta_2 educ_i + \varepsilon_i
\end{gather}

For Cobb-Douglas functional form, the output elasticities of the inputs are equal to the corresponding coefficients $\beta_1$ and $\beta_2$. \\

The translog production function (already in log and added to an error term) has the following form: 

\begin{equation}
\label{eq:translog.nocontrols}
\begin{aligned}
comp_i &= \alpha + \beta_1 hexp_i + \beta_2 educ_i + \beta_3 hexp_i^2 + \beta_4 educ_i^2 + \beta_5 \left(hexp_i \times educ_i\right) + \varepsilon_i
\end{aligned}
\end{equation}

% We don't talk about it anywhere else
%The truncated translog funtion, which has been widely used in the literature\footnote{This is the functional form chosen by Greene (2004) because (1) the full translog presented large diseconomies of scale, (2) the full translog was not monotonic in the inputs for all values, thereby allowing a "much looser interpretation" of the production relationship and (3) the truncated translog functional form allowed continuity with the literature.}, has the following form:

Table \ref{tab:func.form} compares the simplest estimates of the two functional forms above. These preliminary estimates suggest that the translog specification may be suffering from overfitting, as the added variables have no statistical significance. The results are nevertheless consistent with the theory. We proceed to test their fit.

<<include=TRUE,results='asis',echo=FALSE,eval=TRUE>>=
stargazer(prod.CD,prod.TL, font.size = "small", dep.var.labels.include = FALSE, dep.var.caption = c("Dependent variable = comp"), column.labels = c("Cobb-Douglas","Translog"), se=list(NULL,NULL,NULL), covariate.labels = c('hexp', 'educ', 'hexp$^2$', 'educ$^2$', 'hexp*edux', 'Constant'), df = FALSE, title = 'Regression results for Cobb-Douglas and translog in the cross section 1997', label = 'tab:func.form')
#omit.stat = c("f","ser"), 
@

\subsubsection{Constant Elasticity of Substitution}
We also consider a Constant Elasticity of Substitution (CES) functional form, with COMP as the dependent variable and HEXP and EDUC as the explanatory variables.
\begin{equation}
comp_i = \gamma \left( \delta \times hexp_i^{-\rho} + (1- \delta) \times educ_i^{-\rho} \right)^{-\frac{\nu}{\rho}}
\end{equation}
where we set $\nu = 1$ if we assume constant returns of scale, and the elasticity is given by $\epsilon = \frac{1}{1- \rho}$. The CES model estimated with data for 1997 gives unrealistically high elasticities of substitution, with too large standard errors. We Therefore, we chose not to use it. The same happens in the pooled case. The estimates are presented in Table \ref{tab:ces}.
<<CES, eval=T, echo=F, warning=FALSE, results='hide', include=FALSE>>=
library(micEconCES)
cont.vars <- c("HEXP", "EDUC", "GINI", "POPDEN", "GDPC")
disc.vars <- c("COUNTRYCODE", "YEAR", "TROPICS", "VOICE", "OECD", "GEFF", "PUBTHE")
dep.vars <- c("COMP", "DALE")
l.df <- cbind(log(df[,c(dep.vars, cont.vars)]), df[c(disc.vars)])
l.df97 <- subset(l.df, YEAR == 1997)
### Cross-sectional CES for 1997
# Constant Elasticity
ces.comp <- cesEst("COMP", c("HEXP", "EDUC"), l.df97, method = "Kmenta", vrs = F)
ces.comp.vrs <- cesEst("COMP", c("HEXP", "EDUC"), l.df97, method = "Kmenta", vrs = T)
ces.comp2 <- cesEst("COMP", c("HEXP", "EDUC"), l.df, method = "Kmenta")
ces.comp2.vrs <- cesEst("COMP", c("HEXP", "EDUC"), l.df, method = "Kmenta", vrs = T)
out <- rbind.data.frame(summary(ces.comp)$ela, summary(ces.comp)$coefficients,
                        summary(ces.comp.vrs)$ela, summary(ces.comp.vrs)$coefficients,
                        summary(ces.comp2)$ela, summary(ces.comp2)$coefficients,
                        summary(ces.comp2.vrs)$ela, summary(ces.comp2.vrs)$coefficients)
tmp <- rep(c("$\\epsilon$", "$\\gamma$", "$\\delta$", "$\\rho$",
             "$\\epsilon$", "$\\gamma$", "$\\delta$", "$\\rho$", "$\\nu$"), 2)
out <- cbind.data.frame(Model = c("Cross-Section", NA, NA, NA,
                                  "Cross-Section,", "variable returns of scale", NA, NA, NA,
                                  "Pooled OLS", NA, NA, NA,
                                  "Pooled OLS,", "variable returns of scale", NA, NA, NA),
                                  tmp, out)
rownames(out) <- NULL
names(out) <- c("Model", "Parameter", "Estimate", "s.e.", "t-stat.", "p-value")
@
<<CEStable, eval=T, echo=F, warning=FALSE, results='asis'>>=
print(xtable(out, caption = "Estimates of the CES production function",
        label = "tab:ces", auto = T,  digits = 3), include.rownames=FALSE, booktabs = TRUE,
        hline.after = c(-1, 0, 4, 9, 13, 18),
        sanitize.text.function = function(x) {x},
        add.to.row = list(pos = list(18), command = paste0("\\midrule \n \\multicolumn{5}{l}",
        "{\\footnotesize{Note: $\\epsilon$ denotes the elasticity of substitution}}",
        " \\\\ \n")))

@


\subsubsection{Tests on the functional forms}
Since "the Cobb-Douglas production function is nested in the translog production function, we can apply a Wald test or Likelihood Ratio test to check whether the Cobb-Douglas production function is rejected in favor of the translog production function" (Henningsen, 2014). Tables \ref{WT} and \ref{LR} show the results of Wald and Likelihood Ratio tests between the Cobb-Douglas and the translog functional forms. The null hypothesis is $$H_0: \beta_3 = \beta_4 = \beta_5 = 0$$, where the coefficients come from Equation \ref{eq:translog.nocontrols}. For both tests the null cannot be rejected. Therefore, in the interest of parsimony, we adopt the Cobb-Douglas specification.% THE SAME TESTS WITH THE AGREED CONTROLS POINT TO TL, BUT THEN EDUC IS NOT SIGNIFICANT!

<<include=TRUE,results='asis',echo=FALSE,eval=TRUE>>=
print(xtable(WT.CDvTL, caption = c("Wald test, $H_0: \\beta_3 = \\beta_4 = \\beta_5 = 0$"), label= "WT"), caption.placement="top", include.rownames=FALSE)
print(xtable(LR.CDvTL, caption = c("LR Test, $H_0: \\beta_3 = \\beta_4 = \\beta_5 = 0$"), label= "LR"), caption.placement="top", include.rownames=FALSE)
#print(xtable(WT.CDvTL.contr, caption = c("Wald Test: Cobb-Douglas vs Translog (incl. controls); sign. difference"), label= "WT with controls"), caption.placement="top")
#print(xtable(LR.CDvTL.contr, caption = c("LR Test: Cobb-Douglas vs Translog (incl. controls); sign. difference"), label= "LR with controls"), caption.placement="top")
@

<<include=F,results='asis',echo=FALSE,eval=F>>=
print(xtable(WT.CDvTL, caption = c("Wald Test: Cobb-Douglas vs Translog; no sign. difference")), caption.placement="top")
print(xtable(WT.CDvTTL, caption = c("Wald Test: Cobb-Douglas vs Truncated Translog; no sign. difference")), caption.placement="top")
print(xtable(WT.TTLvTL, caption = c("Wald Test: Truncated Translog vs Translog; no sign. difference")), caption.placement="top")
print(xtable(LR.CDvTL, caption = c("LR Test: Cobb-Douglas vs Translog; no sign. difference")), caption.placement="top")
print(xtable(LR.CDvTTL, caption = c("LR Test: Cobb-Douglas vs Truncated Translog; no sign. difference")), caption.placement="top")
print(xtable(LR.TTLvTL, caption = c("LR Test: Truncated Translog vs Translog; no sign. difference")), caption.placement="top")
print(xtable(WT.CDvTL.contr, caption = c("Wald Test: Cobb-Douglas vs Translog (incl. controls); sign. difference")), caption.placement="top")
print(xtable(WT.CDvTTL.contr, caption = c("Wald Test: Cobb-Douglas vs Tr. Translog (incl. controls); no sign. diff.")), caption.placement="top")
print(xtable(WT.TTLvTL.contr, caption = c("Wald Test: Tr. Translog vs Translog (incl. controls); sign. difference")), caption.placement="top")
print(xtable(LR.CDvTL.contr, caption = c("LR Test: Cobb-Douglas vs Translog (incl. controls); sign. difference")), caption.placement="top")
print(xtable(LR.CDvTTL.contr, caption = c("LR Test: Cobb-Douglas vs Tr. Translog (incl. controls); no sign. diff.")), caption.placement="top")
print(xtable(LR.TTLvTL.contr, caption = c("LR Test: Truncated Translog vs Translog (incl. controls); sign. difference")), caption.placement="top")
@

\subsubsection{Fitted values analysis}

In Figure \ref{fig:modselection_resdistribution}, Cobb-Douglas and translog residuals resemble a normal distribution centered at 0. Nevertheless, both present a fatter left tail and a mean slightly above 0. In the Q-Q plots, the sample quantiles depart from the normal quantiles underlining the fatter left tail of the distribution. Overall, the Cobb-Douglas residuals seem to do a better job in satisfying the normality assumption.

<<modselection_resdistribution,include=TRUE,echo=FALSE,eval=TRUE,fig.cap='Kernel density and Q-Q plots of residuals',fig.align='center',out.width='0.7\\textwidth',fig.pos='htbp'>>=
x <- seq(-0.3,0.3,length=1000)
y1 <- dnorm(x, mean=0, sd=0.0626)

par(mfrow=c(2,2))
plot(density(prod.CD$residuals,'ucv'), main='Cobb-Douglas density', ylim=c(0,8),  xlim=c(-0.30,0.22))
lines(x, y1, lty=2)
legend("topleft", c("Residuals","Normal (0, 0.004)"), lty = c(1,2), cex = 0.75)
plot(density(prod.TL$residuals,'ucv'), main='Translog density', ylim=c(0,8),  xlim=c(-0.30,0.22))
lines(x, y1, lty=2)
#plot(density(prod.CD.contr$residuals, 'ucv'), main='Cobb-Douglas with controls')  # !!!!!!!!!!! BANDWIDTH !!!!!!!!!!!!
#plot(density(prod.TL.contr$residuals,'ucv'), main='Translog with controls')
qqnorm(prod.CD$residuals, xlab = 'Normal Quantiles', ylab = 'Sample Quantiles', main='Cobb-Douglas Q-Q plot')
 qqline(prod.CD$residuals)
qqnorm(prod.TL$residuals, xlab = 'Normal Quantiles', ylab = 'Sample Quantiles', main='Translog Q-Q plot')
 qqline(prod.TL$residuals)
@

<<modselection_qqplots,include=F,echo=FALSE,eval=F,fig.cap='Q-Q plots of residuals',fig.align='center',out.width='0.7\\textwidth',fig.pos='htbp'>>=
par(mfrow=c(2,2))
qqnorm(prod.CD$residuals, xlab = 'Normal Quantiles', ylab = 'Sample Quantiles', main='Cobb-Douglas')
 qqline(prod.CD$residuals)
qqnorm(prod.TL$residuals, xlab = 'Normal Quantiles', ylab = 'Sample Quantiles', main='Translog')
 qqline(prod.TL$residuals)
qqnorm(prod.CD.contr$residuals, xlab = 'Normal Quantiles', ylab = 'Sample Quantiles', main='Cobb-Douglas with controls')
 qqline(prod.CD.contr$residuals)
qqnorm(prod.TL.contr$residuals, xlab = 'Normal Quantiles', ylab = 'Sample Quantiles', main='Translog with controls')
 qqline(prod.TL.contr$residuals)
@



Figure \ref{fig:modselection_resvsfitted} shows the residuals and the squared residuals plotted against the fitted values. The solid line is a nonparametric smooth spline that captures the trend in the data. The residuals for both specifications do not present any systematic trend, as the spline runs straight and horizontal around zero. From the observation of the upper two figures, however, the heteroskedasticity of the residuals becomes evident, as the variance of the residuals seems to decline with increasing fitted values. The figures of squared residuals provide further evidence for the heteroskedasticity problem.

<<modselection_resvsfitted,include=TRUE,echo=FALSE,eval=TRUE,fig.cap='Plot of fitted values and residuals (squared)',fig.align='center',out.width='0.7\\textwidth',fig.pos='htbp'>>=
par(mfrow=c(2,2))
plot(prod.CD$fitted, prod.CD$residuals, xlab = 'Fitted Values', ylab = 'Residuals', main='Cobb-Douglas residuals')
  abline(h=0, lty=2)
  lines(smooth.spline(prod.CD$fitted, prod.CD$residuals))
plot(prod.TL$fitted, prod.TL$residuals, xlab = 'Fitted Values', ylab = 'Residuals', main='Translog residuals')
  abline(h=0, lty=2)
  lines(smooth.spline(prod.TL$fitted, prod.TL$residuals))
#plot(prod.CD.contr$fitted, prod.CD.contr$residuals, xlab = 'Fitted Values', ylab = 'Residuals', main='Cobb-Douglas with controls')
  #abline(h=0, lty=2)
  #lines(smooth.spline(prod.CD.contr$fitted, prod.CD.contr$residuals))
#plot(prod.TL.contr$fitted, prod.TL.contr$residuals, xlab = 'Fitted Values', ylab = 'Residuals', main='Translog with controls')
  #abline(h=0, lty=2)
  #lines(smooth.spline(prod.TL.contr$fitted, prod.TL.contr$residuals))
plot(prod.CD$fitted, prod.CD$residuals^2, xlab = 'Fitted Values', ylab = 'Squared Residuals', main='Cobb-Douglas squared residuals')
  lines(smooth.spline(prod.CD$fitted, prod.CD$residuals^2))
plot(prod.TL$fitted, prod.TL$residuals^2, xlab = 'Fitted Values', ylab = 'Squared Residuals', main='Translog squared residuals')
  lines(smooth.spline(prod.TL$fitted, prod.TL$residuals^2))
@

<<modselection_ressqvsfitted,include=F,echo=FALSE,eval=F,fig.cap='Plot of fitted values and residuals squared',fig.align='center',out.width='0.7\\textwidth',fig.pos='htbp'>>=
par(mfrow=c(2,2))
plot(prod.CD$fitted, prod.CD$residuals^2, xlab = 'Fitted Values', ylab = 'Residuals', main='Cobb-Douglas')
  lines(smooth.spline(prod.CD$fitted, prod.CD$residuals^2))
plot(prod.TL$fitted, prod.TL$residuals^2, xlab = 'Fitted Values', ylab = 'Residuals', main='Translog')
  lines(smooth.spline(prod.TL$fitted, prod.TL$residuals^2))
plot(prod.CD.contr$fitted, prod.CD.contr$residuals^2, xlab = 'Fitted Values', ylab = 'Residuals', main='Cobb-Douglas with controls')
  lines(smooth.spline(prod.CD.contr$fitted, prod.CD.contr$residuals^2))
plot(prod.TL.contr$fitted, prod.TL.contr$residuals^2, xlab = 'Fitted Values', ylab = 'Residuals', main='Translog with controls')
  lines(smooth.spline(prod.TL.contr$fitted, prod.TL.contr$residuals^2))
@



Finally, Figure \ref{fig:modselection_resvsCOMP} shows residuals plotted against input and output variables. The nonparametric solid line captures the trend in the data. The two upper plots, which include residuals and the output variable comp, show evidence of a systematic trend. The increasing shape in the residuals is common to both Cobb-Douglas and translog specification and hints to some bias, since for low values of comp, the model constantly predicts too high fitted values, i.e. negative residuals. The plots which include hexp are less problematic and the Cobb-Douglas specification look more independent than the translog specification with respect to the variable educ. In all plots, the change in spread of the residuals' distribution suggests heterogeneity.

<<modselection_resvsCOMP,include=TRUE,echo=FALSE,eval=TRUE,fig.cap='Plot of comp, hexp, educ and residuals',fig.align='center',out.width='0.7\\textwidth',fig.pos='htbp'>>=
par(mfrow=c(3,2))
#plot(COMP, prod.CD$residuals, xlab = 'COMP', ylab = 'Residuals', main='Cobb-Douglas')
#  abline(h=0, lty=2)
#  lines(smooth.spline(COMP, prod.CD$residuals))
#plot(COMP, prod.TL$residuals, xlab = 'COMP', ylab = 'Residuals', main='Translog')
#  abline(h=0, lty=2)
#  lines(smooth.spline(COMP, prod.TL$residuals))
plot(comp, prod.CD$residuals, xlab = 'comp', ylab = 'Residuals', main='Cobb-Douglas residuals')
  abline(h=0, lty=2)
  lines(smooth.spline(comp, prod.CD$residuals))
plot(comp, prod.TL$residuals, xlab = 'comp', ylab = 'Residuals', main='Translog residuals')
  abline(h=0, lty=2)
  lines(smooth.spline(comp, prod.TL$residuals))
#plot(COMP, prod.CD.contr$residuals, xlab = 'COMP', ylab = 'Residuals', main='Cobb-Douglas with controls')
  #abline(h=0, lty=2)
  #lines(smooth.spline(COMP, prod.CD.contr$residuals))
#plot(COMP, prod.TL.contr$residuals, xlab = 'COMP', ylab = 'Residuals', main='Translog with controls')
  #abline(h=0, lty=2)
  #lines(smooth.spline(COMP, prod.TL.contr$residuals))
plot(log(HEXP), prod.CD$residuals, xlab = 'hexp', ylab = 'Residuals', main='Cobb-Douglas residuals')
  abline(h=0, lty=2)
  lines(smooth.spline(log(HEXP), prod.CD$residuals))
plot(log(HEXP), prod.TL$residuals, xlab = 'hexp', ylab = 'Residuals', main='Translog residuals')
  abline(h=0, lty=2)
  lines(smooth.spline(log(HEXP), prod.TL$residuals))
plot(log(EDUC), prod.CD$residuals, xlab = 'educ', ylab = 'Residuals', main='Cobb-Douglas residuals')
  abline(h=0, lty=2)
  lines(smooth.spline(log(EDUC), prod.CD$residuals))
plot(log(EDUC), prod.TL$residuals, xlab = 'educ', ylab = 'Residuals', main='Translog residuals')
  abline(h=0, lty=2)
  lines(smooth.spline(log(EDUC), prod.TL$residuals))
@

<<modselection_resvslogHEXP,include=F,echo=FALSE,eval=F,fig.cap='Plot of log(HEXP) and residuals',fig.align='center',out.width='0.7\\textwidth',fig.pos='htbp'>>=
par(mfrow=c(2,2))
plot(log(HEXP), prod.CD$residuals, xlab = 'log(HEXP)', ylab = 'Residuals', main='Cobb-Douglas')
  abline(h=0, lty=2)
  lines(smooth.spline(log(HEXP), prod.CD$residuals))
plot(log(HEXP), prod.TL$residuals, xlab = 'log(HEXP)', ylab = 'Residuals', main='Translog')
  abline(h=0, lty=2)
  lines(smooth.spline(log(HEXP), prod.TL$residuals))
plot(log(HEXP), prod.CD.contr$residuals, xlab = 'log(HEXP)', ylab = 'Residuals', main='Cobb-Douglas with controls')
  abline(h=0, lty=2)
  lines(smooth.spline(log(HEXP), prod.CD.contr$residuals))
plot(log(HEXP), prod.TL.contr$residuals, xlab = 'log(HEXP)', ylab = 'Residuals', main='Translog with controls')
  abline(h=0, lty=2)
  lines(smooth.spline(log(HEXP), prod.TL.contr$residuals))
@

<<modselection_resvslogEDUC,include=F,echo=FALSE,eval=F,fig.cap='Plot of log(EDUC) and residuals',fig.align='center',out.width='0.7\\textwidth',fig.pos='htbp'>>=
par(mfrow=c(2,2))
plot(log(EDUC), prod.CD$residuals, xlab = 'log(EDUC)', ylab = 'Residuals', main='Cobb-Douglas')
  abline(h=0, lty=2)
  lines(smooth.spline(log(EDUC), prod.CD$residuals))
plot(log(EDUC), prod.TL$residuals, xlab = 'log(EDUC)', ylab = 'Residuals', main='Translog')
  abline(h=0, lty=2)
  lines(smooth.spline(log(EDUC), prod.TL$residuals))
plot(log(EDUC), prod.CD.contr$residuals, xlab = 'log(EDUC)', ylab = 'Residuals', main='Cobb-Douglas with controls')
  abline(h=0, lty=2)
  lines(smooth.spline(log(EDUC), prod.CD.contr$residuals))
plot(log(EDUC), prod.TL.contr$residuals, xlab = 'log(EDUC)', ylab = 'Residuals', main='Translog with controls')
  abline(h=0, lty=2)
  lines(smooth.spline(log(EDUC), prod.TL.contr$residuals))
@

<<include=FALSE,eval=FALSE>>=
# NOT USED
# Linear model
prod.lin <- lm(COMP ~ HEXP + EDUC, data = df97)
summary(prod.lin)

# Quadratic model
prod.quad <- lm(COMP ~ HEXP + EDUC + I(0.5*hexp^2) + I(0.5*educ^2)
                + I(HEXP*EDUC), data = df97)
summary(prod.quad)

# Wald and LR tests
waldtest(prod.lin, prod.quad)  # significant difference -> quad
lrtest(prod.lin, prod.quad)    # significant difference -> quad

# Visual inspection of goodness of fit
fitted.quad <- fitted(prod.quad)
compPlot(COMP, fitted.quad)
fitted.CD <- fitted(prod.CD)
compPlot(log(COMP), fitted.CD)

# Hypothetical R-squared
summary(prod.quad)$r.squared
rSquared(COMP, COMP - exp(fitted.CD))  # hyp R-sq of quadratic

summary(prod.CD)$r.squared
rSquared(log(COMP), log(COMP) - log(fitted.quad))
@

<<include=F, eval=TRUE>>=
detach(df97)
@



\subsection{Choice of controls}
Having chosen the variable COMP as the dependant variable, and the Cobb-Douglas production function, we turn our attention to the choice of control variables. Table \ref{tab:cor} shows a high correlation between some of the possible control variables. We need to choose among some of them to preserve the interpretability of the coeficients. Using GDPC would make the identification of the effect of HEXP difficult, for example, so we discard it. We proceeded by including the most uncorrelated combination of variables and inspecting the fit of the resulting regression. The set of controls chosen is: GINI, GINI$^2$, POPDEN, VOICE, OECD.
<<correlation, echo = F, results='asis'>>=
tmp <- cor(df[, 4:15])
tmp[lower.tri(tmp, diag = F)] <- NA
print(xtable(tmp, label = 'tab:cor', caption = 'Correlation matrix of the data'),
      size = "scriptsize", rotate.colnames = TRUE)
@

<<include=F,eval=F>>=
# Selection of the controls
summary(prod.TL.contr)

# Translog plus controls plus gini squared
prod.TL.gini2 <- lm(comp ~ hexp + educ
                    + I(hexp^2) + I(educ^2) 
                    + I(hexp*educ)
                    + popden + PUBTHE + VOICE + gini + I(gini^2) + TROPICS + GEFF + gdpc, data = df97)
summary(prod.TL.gini2)

# Translog plus controls II
prod.TL.contrII <- lm(comp ~ hexp + educ
                    + I(hexp^2) + I(educ^2) 
                    + I(hexp*educ)
                    + popden + VOICE + gini + I(gini^2) + gdpc, data = df97)
summary(prod.TL.contrII)

# Translog plus controls III
prod.TL.contrIII <- lm(comp ~ hexp + educ
                    + I(hexp^2) + I(educ^2) 
                    + I(hexp*educ)
                    + popden + PUBTHE + VOICE + gini + I(gini^2), data = df97)
summary(prod.TL.contrIII)

# Translog plus controls IIII
prod.TL.contrIIII <- lm(comp ~ hexp + educ
                    + I(hexp^2) + I(educ^2) 
                    + I(hexp*educ)
                    + popden + PUBTHE + VOICE + gini + I(gini^2) + gdpc, data = df97)
summary(prod.TL.contrIIII)

# Translog plus controls IIIII
prod.TL.contrIIIII <- lm(comp ~ hexp + educ
                    + I(hexp^2) + I(educ^2) 
                    + I(hexp*educ)
                    + PUBTHE + VOICE + gini + I(gini^2) + gdpc, data = df97)
summary(prod.TL.contrIIIII)

# Robust standard errors
robust.se.TL.contr <- sqrt(diag(vcovHC(prod.TL.contr, type="HC1")))
robust.se.TL.contrIII <- sqrt(diag(vcovHC(prod.TL.contrIII, type="HC1")))

@

<<include=F,results='asis',echo=FALSE,eval=F>>=
stargazer(prod.TL.contr,prod.TL.gini2,prod.TL.contrII,prod.TL.contrIII,prod.TL.contrIIII,prod.TL.contrIIIII, font.size = "scriptsize", dep.var.labels.include = FALSE, dep.var.caption = c("Dependent variable = comp"), column.labels = c("TL OLS","TL OLS","TL OLS","TL OLS","TL OLS","TL OLS"), se=list(NULL,NULL,NULL,NULL,NULL,NULL), df = FALSE, title = 'Regr. results for cross-sectional translog with different model spec.', covariate.labels = c('hexp', 'educ', 'hexp$^2$', 'educ$^2$', 'hexp*educ', 'TROPICS', 'GEFF', 'VOICE', 'PUBTHE', 'popden', 'gdpc', 'gini', 'gini$^2$', 'Constant'))
@

<<LR_pooled, include=FALSE,eval=F>>=

#attach(df97)
detach(df97)
attach(df2)
comp <- log(COMP)
dale <- log(DALE)
hexp <- log(HEXP)
educ <- log(EDUC)
gini <- log(GINI)
popden <- log(POPDEN)
gdpc <- log(GDPC)

# Cobb-Douglas
prod.CD.pooled <- lm(comp ~ hexp + educ, data = df2)
summary(prod.CD.pooled)
# Cobb-Douglas plus controls
prod.CD.contr.pooled <- lm(comp ~ hexp + educ 
                    + TROPICS + GEFF + VOICE + PUBTHE + popden + gdpc + gini, data = df2)
summary(prod.CD.contr.pooled)

# Translog
prod.TL.pooled <- lm(comp ~ hexp + educ 
              + I(hexp^2) + I(educ^2) 
              + I(hexp*educ), data = df2)
summary(prod.TL.pooled)
# Translog plus controls
prod.TL.contr.pooled <- lm(comp ~ hexp + educ 
                    + I(hexp^2) + I(educ^2) 
                    + I(hexp*educ)
                   + TROPICS + GEFF + VOICE + PUBTHE + popden + gdpc + gini, data = df2)
summary(prod.TL.contr.pooled)

# Truncated Translog
prod.TTL.pooled <- lm(comp ~ hexp + educ
               + I(educ^2), data = df2)
summary(prod.TTL.pooled)
# Truncated Translog plus controls
prod.TTL.contr.pooled <- lm(comp ~ hexp + educ
                     + I(educ^2)
                    + TROPICS + GEFF + VOICE + PUBTHE + popden + gdpc + gini, data = df2)
summary(prod.TTL.contr.pooled)

# Wald and LR tests
WT.CDvTL.pooled <- waldtest(prod.CD.pooled, prod.TL.pooled)   # significant difference -> TL
WT.CDvTTL.pooled <- waldtest(prod.CD.pooled, prod.TTL.pooled)  # no significant difference -> CD
WT.TTLvTL.pooled <- waldtest(prod.TTL.pooled, prod.TL.pooled)  # significant difference -> TL
LR.CDvTL.pooled <- lrtest(prod.CD.pooled, prod.TL.pooled)   # significant difference -> TL
LR.CDvTTL.pooled <- lrtest(prod.CD.pooled, prod.TTL.pooled)  # no significant difference -> CD
LR.TTLvTL.pooled <- lrtest(prod.TTL.pooled, prod.TL.pooled)  # significant difference -> TL

WT.CDvTL.contr.pooled <- waldtest(prod.CD.contr.pooled, prod.TL.contr.pooled)   # significant difference -> TL (same result if we drop POPDEN)
WT.CDvTTL.contr.pooled <- waldtest(prod.CD.contr.pooled, prod.TTL.contr.pooled)  # no significant difference -> CD
WT.TTLvTL.contr.pooled <- waldtest(prod.TTL.contr.pooled, prod.TL.contr.pooled)  # significant difference -> TL
LR.CDvTL.contr.pooled <- lrtest(prod.CD.contr.pooled, prod.TL.contr.pooled)   # significant difference -> TL (same result if we drop POPDEN)
LR.CDvTTL.contr.pooled <- lrtest(prod.CD.contr.pooled, prod.TTL.contr.pooled)  # no significant difference -> CD
LR.TTLvTL.contr.pooled <- lrtest(prod.TTL.contr.pooled, prod.TL.contr.pooled)  # significant difference -> TL

# Constant elasticities of substitution
#c("HEXP", "EDUC")
#prod.CES <- 
#summary(prod.CES)

detach(df2)
@

<<TTL_pooled,include=FALSE,eval=F>>=
TTL <- lm(log(df97$COMP) ~ log(df97$HEXP) + log(df97$EDUC)
                     + I(log(df97$EDUC)^2) + I(log(df97$HEXP)*log(df97$EDUC))
                     + df97$VOICE + df97$PUBTHE + log(df97$POPDEN) + log(df97$GDPC) + log(df97$GINI) + I(log(df97$GINI)^2), data = df97)
summary(TTL)

TL <- lm(log(df97$COMP) ~ log(df97$HEXP) + log(df97$EDUC)
                     + I(log(df97$HEXP)^2) + I(log(df97$EDUC)^2) + I(log(df97$HEXP)*log(df97$EDUC))
                     + df97$VOICE + df97$PUBTHE + log(df97$POPDEN) + log(df97$GDPC) + log(df97$GINI) + I(log(df97$GINI)^2), data = df97)
summary(TL)

CD <- lm(log(df97$COMP) ~ log(df97$HEXP) + log(df97$EDUC) 
                    + df97$VOICE + df97$PUBTHE + log(df97$POPDEN) + log(df97$GDPC) + log(df97$GINI) + I(log(df97$GINI)^2), data = df97)
summary(CD)


lrtest(CD, TTL)
lrtest(TTL, TL)

@

\section{Estimation and results}

<<setup, eval=TRUE, echo=F, include=F, cache = F>>=
### Code for the empirical project
# Setup

#rm(list = ls())  # NOT NECESSARY
#setwd("/Users/custodioij/Dropbox/TSE/M2/M2 Empirical Project/R code (Igor)/Meeting 8_03")  # NOT NECESSARY

# library(gdata)
# library(stats4)
# library(bbmle)
library(micEcon)
# library(micEconCES)
# library(ks)
library(stargazer)
library(xtable)
# library(sandwich)
# library(lmtest)
library(plm)  # Panel data
library(ggplot2)
library(reshape2)
# library(np)
# library(quantreg)
# erfc <- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE) # error function
# tl.to.df <- function(tl){ # formats the data somehow (?)
#         nomes <- tl$xNames
#         nomes.coef <- c("Intercept", nomes)
#         for (i in 1:length(nomes)){
#                 tmp <- paste(nomes[i], "X", nomes[i:length(nomes)])
#                 nomes.coef <- c(nomes.coef, tmp)
#         }
#         tab <- summary(tl)$coefTable
#         attr(tab, "dimnames")[[1]] <- nomes.coef
#         return(tab)
# }
# theme_set(theme_bw()) ## Sets ggplot theme
@

<<data, eval=TRUE, echo=F, cache=TRUE>>=
# Import data

# df = read.xls("Data/who_data_cc.xlsx", sheet = 1, header = TRUE)[-c(701),]  # ALREADY LOADED WITH SAME NAME

names(df)[length(names(df))] <- "GDPC"
names(df)[which(names(df) == "COUNTRYNAME")] <- "COUNTRY"
# Ireland has an error in 1993
# df <- subset(df, COUNTRYCODE != (df[which(df$COMP == 1),]$COUNTRYCODE))  # NOT NECESSARY
df$COUNTRY <- factor(df$COUNTRY)
# df$YEAR <- factor(df$YEAR)
# df97 <- subset(df, YEAR == 1997)  # ALREADY LOADED WITH SAME NAME
cont.vars <- c("HEXP", "EDUC", "GINI", "POPDEN", "GDPC")
disc.vars <- c("COUNTRY", "YEAR", "TROPICS", "VOICE", "OECD", "GEFF", "PUBTHE")
dep.vars <- c("COMP", "DALE")
l.df <- cbind(log(df[c(dep.vars, cont.vars)]), df[disc.vars])
l.df97 <- subset(l.df, YEAR == 1997)
@

We estimate the Cobb-Douglas and the translog model for the cross-sectional case. We then run the former model for the whole panel, with a pooled OLS, a Fixed Effects model and a Random Effects model. Results are reported in Table \ref{tab:big}. We report White standard errors for the first two models, and clustered standard errors for the other three. The results are consistent with the theory: all coefficients for HEXP and EDUC are positive. Moreover, the controls have the expected sings (except for OECD in the Cobb-Douglas and Pooled specifications). Time dummies also are associated to positive coefficients, as expected. The Fixed Effects model does not provide a good fit to the data, as it relies on within-variation and our controls are only available for a single year. The Random Effects model provide a much better fit and makes economic sense. We focus our analysis on it.
%The estimated model is:
% \begin{equation}
% \begin{aligned}
% COMP_i &= \alpha + \beta_1 HEXP_i + \beta_2 EDUC_i + \beta_3 HEXP_i^2 + \beta_4 EDUC_i^2 + \beta_5 \left(HEXP_i \times EDUC_i\right) \\ &+ \gamma X_i + \xi YEAR_i + \varepsilon_i
% \end{aligned}
% \end{equation}
% Where $$ X_i = \left[ POPDEN_i, PUBTHE_i, VOICE_i, GINI_i, GINI_i^2 \right]^\top$$ and YEAR can either be a vector of dummies for each year of the sample (and $\xi$ also a vector of coefficients), or the YEAR itself (and $\xi$ a single coefficient). Again, the values of COMP, HEXP, EDUC, GINI and POPDEN are in logs.
<<TransLog, eval=TRUE, echo=F, results='asis', warning=FALSE>>=
## Trying t keep the trans-log for the two main explanatory variable
trans.log.s <- "COMP ~ HEXP + EDUC + HEXP:EDUC + I(HEXP^2) + I(EDUC^2)"

trans.log.s <- paste(trans.log.s, "OECD + POPDEN + VOICE + GINI + I(GINI^2)",
                     sep = " + ")

cb <- "COMP ~ HEXP + EDUC"
cb <- paste(cb, "OECD + POPDEN + VOICE + GINI + I(GINI^2)",
                     sep = " + ")

cb <- "COMP ~ HEXP + EDUC"
cb <- paste(cb, "GINI + I(GINI^2) + POPDEN + VOICE + OECD",
                     sep = " + ")
# CS
tl1 <- lm(as.formula(trans.log.s), data = l.df97)
cb1 <- lm(as.formula(cb), data = l.df97)
# Pooled
tl2 <- lm(as.formula(paste(trans.log.s, "+ YEAR")), data = l.df)
cb2 <- lm(as.formula(paste(cb, "+ YEAR")), data = l.df)
nonpanel <- list(tl1, cb1, tl2, cb2)
# summary(tl2)
se.nonpanel <- sapply(nonpanel,
                      function(x){sqrt(diag(vcovHC(x, type="HC0")))})
@
% dens <- subset(df[order(df$POPDEN),c('YEAR', 'POPDEN', 'COUNTRY', 'GDPC', 'OECD')], YEAR == 1997) # Makes no sense
<<panel, cache =F, eval=T, echo=F, results='asis'>>=
FE.form <- as.formula(paste(trans.log.s, "+ YEAR + COUNTRY"))
RE.form <- as.formula(paste(trans.log.s, "+ YEAR"))
FE.form.cb <- as.formula(paste(cb, "+ YEAR + COUNTRY"))
RE.form.cb <- as.formula(paste(cb, "+ YEAR"))

FE <- plm(FE.form, data = l.df, index = c("COUNTRY", "YEAR"), model = "within")
FEcb <- plm(FE.form.cb, data = l.df, index = c("COUNTRY", "YEAR"), model = "within")
# FE2 <- plm(FE.form, data = l.df, index = c("COUNTRY"), model = "within")
RE <- plm(RE.form, data = l.df, index = c("COUNTRY", "YEAR"), model = "random")
REcb <- plm(RE.form.cb, data = l.df, index = c("COUNTRY", "YEAR"), model = "random")
# vcov=vcovHC(model.plm,type="HC0",cluster="group")
se.FE <- sqrt(diag(vcovHC(FE, type="HC0", cluster="group")))
se.RE <- sqrt(diag(vcovHC(RE, type="HC0", cluster="group")))
se.FE.cb <- sqrt(diag(vcovHC(FEcb, type="HC0", cluster="group")))
se.RE.cb <- sqrt(diag(vcovHC(REcb, type="HC0", cluster="group")))
panel <- list(FE, FEcb, RE, REcb)
se.panel <- sapply(panel,
                      function(x){sqrt(diag(vcovHC(x, type="HC0", cluster="group")))})
haustest <- phtest(FEcb, REcb)

# Likelihood ratio test = 2 ∗ (log likelihood unconstrained − log likelihood constrained)
# LL.FE <- sum(dnorm(resid(FE), mean=0, sd=(sqrt(var(FE$residuals)) * sqrt((length(FE$residuals)-length(FE$coefficients))/length(FE$residuals))), log=TRUE))
# LL.FEcb <- sum(dnorm(resid(FEcb), mean=0, sd=(sqrt(var(FEcb$residuals)) * sqrt((length(FEcb$residuals)-length(FEcb$coefficients))/length(FEcb$residuals))), log=TRUE))
# LR.FE = 2 * (LL.FE - LL.FEcb)
# LR.FE
# qchisq(0.95,3)  # Treshold value
# 1-pchisq(LR.FE,3)  # p-value
# 
# LL.RE <- sum(dnorm(resid(RE), mean=0, sd=(sqrt(var(RE$residuals)) * sqrt((length(RE$residuals)-length(RE$coefficients))/length(RE$residuals))), log=TRUE))
# LL.REcb <- sum(dnorm(resid(REcb), mean=0, sd=(sqrt(var(REcb$residuals)) * sqrt((length(REcb$residuals)-length(REcb$coefficients))/length(REcb$residuals))), log=TRUE))
# LR.RE = 2 * (LL.RE - LL.REcb)
# LR.RE
# qchisq(0.95,3)  # Treshold value
# 1-pchisq(LR.RE,3)  # p-value

@
<<bigtable, echo = F, results='asis'>>=
models.cb <- c(nonpanel[c(2, 4)], panel[c(2, 4)])
models.tl <- c(nonpanel[c(1, 3)], panel[c(1, 3)])
se.cb <- c(se.nonpanel[c(2, 4)], se.panel[c(2, 4)])
se.tl <- c(se.nonpanel[c(1, 3)], se.panel[c(1, 3)])

# stargazer(cb1, tl1, se=se.nonpanel[2:1],
#         column.labels = c("Cobb-Douglas", "Trans-log"),
#         dep.var.labels.include = FALSE,
#         dep.var.caption  = "Dependent variable = COMP",
#         title = "Regression results, cross-section 1997",
#         label = "tab:cs", #font.size = "scriptsize",
#         model.names = FALSE, df = FALSE,
#         covariate.labels = c('HEXP', 'EDUC', 'HEXP Sq.', 'EDUC Sq.', 'OECD', 'POPDEN',
#                              'VOICE', 'GINI', 'GINI Sq.', 'HEXP X EDUC'))
stargazer(cb1, tl1, cb2, FEcb, REcb,
          se=list(se.nonpanel[[2]], se.nonpanel[[1]], se.nonpanel[[4]], se.FE.cb, se.RE.cb),
        column.labels = c("Cobb-Douglas", "Trans-log", "Pooled",
                          "Fixed Effects", "Random Effects"),
        # column.separate = c(2, 3),
        # dep.var.labels.include = FALSE,
        dep.var.caption  = c("Dependent variable: COMP"),
        multicolumn = FALSE,
        dep.var.labels  = c("1997", "1997", "1993-7", "1993-7", "1993-7"),
        title = "Regression results, whole dataset",
        label = "tab:big", font.size = "scriptsize",
        model.names = FALSE, df = FALSE,
        covariate.labels = c('HEXP', 'EDUC', 'HEXP Sq.', 'EDUC Sq.', 'GINI', 'GINI Sq.',
                             'HEXP X EDUC', 'POPDEN', 'VOICE', 'OECD', 'YEAR (continuous)',
                             'YEAR = 1994','YEAR = 1995', 'YEAR = 1996', 'YEAR = 1997'))
@
% #' <<hausman, echo = F>>=
% #' haustest <- phtest(FEcb, REcb)
% #' @
We also perform a Hausman test to decide between the fixed and random efects model in the Cobb-Douglas specification. The test gives a p-value of \Sexpr{phtest(FEcb, REcb)$p.value}. We do not reject the null hypothesis and therefore we prefer the random effects model.

% \subsection{Marginal Effects}
% Figure \ref{fig:signs} shows the signs of the values of the derivatives of the two main explanatory variables. The derivative of EDUC can only be negative on the Cross-Sectional model, given the values of HEXP from the data. The derivative of HEXP, however, can be negative in both panel models. With the exception of HEXP in the CS and Pooled models, we see growing marginal returns (for the logs of the variables). The marginal effects for the Fixed and Random Effects models include large standard deviations, which is likely the reason for its inconsistency.
<<signs, echo = F, eval = F, fig.cap = "Marginal effects of COMP with respect to HEXP or EDUC, as a function of the other">>=
## Check where the derivative wrt EDUC and HEXP change sign (as a fct of the other)
quant.hexp <- quantile(l.df$HEXP)
quant.educ <- quantile(l.df$EDUC)

hexp1 <- coef(tl1)[c(2, 4, 11)]
hexp2 <- coef(tl2)[c(2, 4, 12)]
hexp3 <- coef(FE)[c(1, 3, 8)]
hexp4 <- coef(RE)[c(2, 4, 15)]

educ1 <- coef(tl1)[c(3, 5, 11)]
educ2 <- coef(tl2)[c(3, 5, 12)]
educ3 <- coef(FE)[c(2, 4, 8)]
educ4 <- coef(RE)[c(3, 5, 15)]

d <- function(level, betas, x) {
        tmp <- sapply(level, function(level, betas, x){
                (betas[1] + 2*x*betas[2] + level*betas[3])}, betas, x)
        return(cbind.data.frame(tmp, x))
}
range.hexp <- seq(min(l.df$HEXP), max(l.df$HEXP), length = 10)
range.educ <- seq(min(l.df$EDUC), max(l.df$EDUC), length = 10)

d.hexp1 <- cbind(d(quant.educ, hexp1, range.hexp), Covar = 'Log HEXP', Model = 'CS')
d.hexp2 <- cbind(d(quant.educ, hexp2, range.hexp), Covar = 'Log HEXP', Model = 'Pooled')
d.hexp3 <- cbind(d(quant.educ, hexp3, range.hexp),
                 Covar = 'Log HEXP', Model = 'Fixed Effects')
d.hexp4 <- cbind(d(quant.educ, hexp4, range.hexp),
                 Covar = 'Log HEXP', Model = 'Random Effects')

d.educ1 <- cbind(d(quant.hexp, educ1, range.educ), Covar = 'Log EDUC', Model = 'CS')
d.educ2 <- cbind(d(quant.hexp, educ2, range.educ), Covar = 'Log EDUC', Model = 'Pooled')
d.educ3 <- cbind(d(quant.hexp, educ3, range.educ),
                 Covar = 'Log EDUC', Model = 'Fixed Effects')
d.educ4 <- cbind(d(quant.hexp, educ4, range.educ),
                 Covar = 'Log EDUC', Model = 'Random Effects')

derivatives <- melt(rbind(d.hexp1, d.hexp2, d.hexp3, d.hexp4,
        d.educ1, d.educ2, d.educ3, d.educ4), c('Covar', 'Model', 'x'))
names(derivatives)[3:5] <- c("Covariate", "Quantiles", "Derivative")

ggplot(derivatives, aes(x = Covariate, y = Derivative,
                shape = Quantiles)) + geom_point() +
        geom_line() + facet_grid(Model ~ Covar, scales = "free") +
        geom_hline(aes(yintercept = 0)) + theme(legend.position = "bottom") +
        scale_colour_discrete(name = "Other covariate (quantiles)")
@
%, out.width='.5\\textwidth'
\subsection{Residuals}
Visual inspection of the residuals of regression Figure \ref{fig:residuals} indicates heteroskedasticity dependant on COMP, HEXP and EDUC. This is consistent with Figure \ref{on_joses_part} on which we can see that higher variations of COMP over time are associated to more developed countries (a combination of higher values for these three variables). This justifies or use of white standard errors.
<<residuals, echo = F, fig.cap = "Residuals' scatterplot", fig.align='center', fig.pos="!htbp", fig.width = 10, fig.height = 5>>=
tmp <- lapply(list(list(cb1, 'CS'), list(tl1, 'Translog'), list(cb2, 'Pooled'),
                   list(FEcb, 'FE'), list(RE, 'RE')),
                function(model){cbind.data.frame(apply(
                        as.data.frame(model[[1]]$model)[,c('COMP', 'HEXP', 'EDUC')],
                        2, as.numeric), Fitted = as.numeric(predict(model[[1]])),
                        res = as.numeric(model[[1]]$residuals), 
                        model = model[[2]])})
{
        tmp2 <- FE$model[,c(1:5, 11, 12)]
        for (i in 0:4){
                tmp2 <- cbind.data.frame(tmp2, as.numeric(tmp2$YEAR == (1993 + i)))
                names(tmp2)[ncol(tmp2)] <- paste(c(1993 + i))
        }
        tmp2 <- cbind(tmp2, inter = tmp2$EDUC*tmp2$HEXP)
        tmp2 <- cbind(tmp2, pred = apply(tmp2[,c(2:5, 9:13)],
                                         1, function(x){sum(x*coef(FE))}))
        tmp2 <- cbind(tmp2, fix.eff = 0)
        alphas <- fixef(FE)
        for (i in 1:nrow(tmp2)){
                tmp2$fix.eff[i] <- alphas[tmp2$COUNTRY[i]]
        }
        tmp2$pred2 <- tmp2$pred + tmp2$fix.eff
} ## Calculating the FE predicted values
{
        tmp3 <- RE$model
        for (i in 0:4){
                tmp3 <- cbind.data.frame(tmp3, as.numeric(tmp3$YEAR == (1993 + i)))
                names(tmp3)[ncol(tmp3)] <- paste(c(1993 + i))
        }
        tmp3 <- cbind(tmp3, inter = tmp3$EDUC*tmp3$HEXP)
        tmp3 <- cbind(tmp3,
                pred = apply(tmp3[,c(2:10, 13:17)], 1, function(x){
                        return(coef(RE)[1] + sum(x*(coef(RE)[-1])))
                        }
                        ))
}
tmp <- do.call("rbind", tmp)
tmp$Fitted[which(tmp$model == 'FE')] <- tmp2$pred2
tmp$Fitted[which(tmp$model == 'RE')] <- tmp3$pred
tmp <- melt(tmp, c("res", "model"))
ggplot(tmp, aes(x = value, y = res)) +
        geom_point() + facet_grid(model ~ variable, scales = "free") +
        labs(x = NULL, y = "Residuals")
@

%% Not sure what this means
% #' <<quantreg, echo =F>>=
% #' # npqreg(tydat = as.data.frame(Y), txdat = as.data.frame(X),
% #' #                exdat = z, tau = 0.25)
% #' z <- seq(floor(min(l.df97[,'HEXP'])), ceiling(max(l.df97[,'HEXP'])), length = 100)
% #' quant.reg <- npqreg(tydat = l.df97$COMP, txdat = l.df97[,'HEXP'], exdat = z)
% #' plot(z, quantile(quant.reg), type = "l")
% #' @
% \section{Production function properties}
% In what follows, we are going to focuse our attention on the results related to the production function. More precisely, we are going to analyse the reaction and the behaviour of the inputs according to the two, previously found, specifications of the production function. 

\subsection{Output elasticities}
In this section we are interested in to see how the output will react if there is an increase of 1\% in the use of any input. Based on cross-sectional results, we may conclude that if the education in a country increases by 1\% the health care attainment will increase on average by 0.0899\%. In what concerns the second input: HEXP, the 1\% increase of this input will lead to a positive change of 0.0761\% in health care attainment. However, if we consider the panel data setting, we observe that the increase of the output will be by 0.02\% higher, as a response to a 1\% increase in years of education. But, an important difference is observed concerning HEXP, which will lead to a very small increase of output, about : 0.0085\%.
<<echo = F>>=
#Cobb Douglas cross-section vs panel  Properties

#comp=hexp+educ+gini+gini2+popden+voice+oecd
####Cobb-Douglas Cross Section
library(ggplot2)
cb<-"COMP ~ HEXP + EDUC + GINI + I(GINI^2) + POPDEN + VOICE + OECD +log(PUBTHE)"
cb1 <- lm(as.formula(cb), data = l.df97)


#Cobb-Douglas Panel

RE.form.cb <- as.formula(paste(cb, "+ YEAR"))
REcb <- plm(RE.form.cb, data = l.df, index = c("COUNTRY", "YEAR"), model = "random")

#output elasticities  = coeff  CS
output_elasticity_EDUC_CS<-coef(cb1)["EDUC"] #0.08995508
output_elasticity_HEXP_CS<-coef(cb1)["HEXP"] #0.07614115

#output elasticities  = coeff  panel
output_elasticity_EDUC_panel<-coef(REcb)["EDUC"] #0.1037372
output_elasticity_HEXP_panel<-coef(REcb)["HEXP"] #0.008592725

@
\subsection{Marginal Products}
In a production function, marginal productivity depends on both input and output quantities. If the health expenditure in a country increases  by one unit the measure of healthcare atteinment of most of the countries will increase on average by less than 0.05 units(Cross-Section), or by less that  0.005 units in the panel setting. If the education will increase by 1 year, for the most of the countries, the health care attainment will increase by beween 0.5 units (in the cross-sectional setting) and by 1.5 units if we consider the panel setting.
<<echo = F, fig.cap= "Histogram of the Marginal Productivity of Education and Health Expenditure">>=
#marginal products CS
marginal_product_HEXP_CS<-coef(cb1)["HEXP"]*
  (subset(df, YEAR == 1997)$COMP/subset(df, YEAR == 1997)$HEXP)
marginal_product_EDUC_CS<-coef(cb1)["EDUC"]*
  (subset(df, YEAR == 1997)$COMP/subset(df, YEAR == 1997)$EDUC)
marginal_product_HEXP_panel<-coef(REcb)["HEXP"]*
  (df$COMP/df$HEXP)
marginal_product_EDUC_panel<-coef(REcb)["EDUC"]*
  (df$COMP/df$EDUC)
#visualize the variation of these marginal products 
tmp <- cbind.data.frame(els = marginal_product_HEXP_CS, variable = "HEXP",
                        model = "Cross-Section (1997)")
tmp <- rbind(tmp, cbind.data.frame(els = marginal_product_EDUC_CS, variable = "EDUC",
                                   model = "Cross-Section (1997)"))
tmp <- rbind(tmp, cbind.data.frame(els = marginal_product_EDUC_panel, variable = "EDUC",
                                   model = "Random-Effects (1993-7)"))
tmp <- rbind(tmp, cbind.data.frame(els = marginal_product_HEXP_panel, variable = "HEXP",
                                   model = "Random-Effects (1993-7)"))
ggplot(tmp, aes(x = els)) + geom_histogram(bins = 30) + ylab("Frequency") +
        facet_grid(model~variable, scale = "free") + xlab("Marginal Productivity")
@
<<echo = F, fig.cap= "Density estimation of the Marginal Productivity of Education and Health Expenditure">>=
ggplot(tmp, aes(x = els)) + geom_density() + ylab("Density") +
        facet_grid(model~variable, scale = "free") + xlab("Marginal Productivity")
@
\subsection{Elasticity of Scale}
<<echo = F>>=
#elasticity of scale CS
# It is the sum of all output elasticities
elasticity_of_scale_CS<-(coef(cb1)["HEXP"])+(coef(cb1)["EDUC"]) #0.1660962

#elasticity of scale panel
# It is the sum of all output elasticities
elasticity_of_scale_CS<-sum(coef(REcb)["HEXP"])+(coef(REcb)["EDUC"]) #0.1123299
@

The elasticity of scale is the sum of all output elasticities. Hence, increasing all  inputs by one percent, for most of countries
the output COMP will increase by between 0.11\% and 0.16\% depending on the cross-section or panel setting consideration.
We can conclude, therefore, that the technology has strong decreasing returns to scale.

\subsection{Marginal Rate of Technical Substitution}
If the average education per capita reduces by 1 unit, the Health expenditure has to increase by 50 units for most of the countries (CS), 500 units (Panel), in order to keep the same health care attainment level.
On the other hand, if the health ependiture decreases by 1 unit, the nb of years of education should decrease by 0.01, which would correspond to 3.65 days.

<<echo = F, fig.cap= "Histogram of the Marginal Rates of Substitution Between Education and Health Expenditure">>=
#marginal rates of technical substitution CS
MRTS_HEXP_to_EDUC_CS<-marginal_product_EDUC_CS/marginal_product_HEXP_CS
MRTS_EDUC_to_HEXP_CS<-marginal_product_HEXP_CS/marginal_product_EDUC_CS
MRTS_HEXP_to_EDUC_panel<-marginal_product_EDUC_panel/marginal_product_HEXP_panel
MRTS_EDUC_to_HEXP_panel<-marginal_product_HEXP_panel/marginal_product_EDUC_panel
#visualization of the variation of these MRTSs
tmp <- cbind.data.frame(els = MRTS_HEXP_to_EDUC_CS, variable = "HEXP to EDUC",
                        model = "Cross-Section (1997)")
tmp <- rbind(tmp, cbind.data.frame(els = MRTS_EDUC_to_HEXP_CS, variable = "EDUC to HEXP",
                                   model = "Cross-Section (1997)"))
tmp <- rbind(tmp, cbind.data.frame(els = MRTS_EDUC_to_HEXP_panel, variable = "EDUC to HEXP",
                                   model = "Random-Effects (1993-7)"))
tmp <- rbind(tmp, cbind.data.frame(els = MRTS_HEXP_to_EDUC_panel, variable = "HEXP to EDUC",
                                   model = "Random-Effects (1993-7)"))
ggplot(tmp, aes(x = els)) + geom_histogram(bins = 30) + ylab("Frequency") +
        facet_grid(model~variable, scale = "free") + xlab("Marginal Rates of Substitution")
@
<<echo = F, fig.cap= "Density Estimation of the Marginal Rates of Substitution Between Education and Health Expenditure">>=
ggplot(tmp, aes(x = els)) + geom_density() + ylab("Density") +
        facet_grid(model~variable, scale = "free") + xlab("Marginal Rates of Substitution")
@

\section{Conclusion}



\newpage
\section{References}
\textbf{Evans D., Tandon A., Murray C. and Lauer J. (2000)} The Comparative Efficiency of National Health Systems in Producing Health: An Analysis of 191 Countries. World Health Organization, GPE Discussion Paper No. 29 \\

\textbf{Greene W. (2004)} Distinguishing between heterogeneity and inefficiency: stochastic frontier analysis of the World Health Organization's panel data on national health care systems. \textit{Health Economics}, 13: 959-980 \\

\textbf{Henningsen A. (2014)} Introduction to Econometric Production Analysis with R. Draft Version \\

\textbf{World Health Organization (2000)} Health Systems: Improving Performance. The World Health Report



\newpage
\section{Appendix}
<<eval=TRUE, echo=F, include=F>>=
attach(df97)
#detach(df97)
@

<<boxplot_DALE_COMP,include=TRUE,echo=FALSE,eval=TRUE,fig.cap='Output variables variation over time',fig.align='center',out.width='0.7\\textwidth',fig.pos='htbp'>>=
par(mfrow=c(2,2))
boxplot(DALE~YEAR,data=df2, main="DALE variation over time", xlab="YEAR", ylab="DALE")
boxplot(COMP~YEAR,data=df2, main="COMP variation over time", xlab="YEAR", ylab="COMP")
boxplot(DALE~YEAR,data=subset(df2,OECD==1), main="DALE over time for OECD countries", xlab="YEAR", ylab="DALE")
boxplot(COMP~YEAR,data=subset(df2,OECD==1), main="COMP over time for OECD countries", xlab="YEAR", ylab="COMP")
@



%\bibliography{myreferences}  % specify reference file to be used
%\bibliographystyle{apalike}  % bibliografy style

%------------------------------------------------------------------%
\end{document}
%------------------------------------------------------------------%